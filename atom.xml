<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rason&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ideajava.com/"/>
  <updated>2020-01-29T14:42:59.822Z</updated>
  <id>http://ideajava.com/</id>
  
  <author>
    <name>rason</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka Replication</title>
    <link href="http://ideajava.com/2020/01/28/kafka-replication/"/>
    <id>http://ideajava.com/2020/01/28/kafka-replication/</id>
    <published>2020-01-28T07:24:24.000Z</published>
    <updated>2020-01-29T14:42:59.822Z</updated>
    
    <content type="html"><![CDATA[<p>之前研究了一下 Redis 的主从复制，现在对比学习一下 Kafka 的数据复制。</p><h3 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h3><p>Kafka 用 Topic 来组织数据，每个 Topic 可以分为多个分区，每个分区可以有多个副本。Kafka 中的副本跟 Redis 类似，也是分为主从副本，主副本负责读写，从副本不处理客户端的请求，只是一个 BackUp    ;当然在 Redis 中主从副本可以做读写分离，但是 Kafka 是不支持读写分离的，也没必要去做读写分离（后面会解析）。</p><p>Kafka 中的副本类型：</p><ul><li><p>Leader 副本：<br>每个分区只有一个 Leader 副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。</p></li><li><p>Follower 副本：<br>除了 Leader 副本之外的都是 Follower 副本。Follower 副本不处理客户端请求，唯一的任务就是从 Leader 那里复制消息，当 Leader 挂了之后，其中一个 Follower 会被提升为 Leader。</p></li></ul><h3 id="ISR"><a href="#ISR" class="headerlink" title="ISR"></a>ISR</h3><p>在 Redis 中，主从副本保持同步是通过主副本向从副本发送命令进行同步的。而 Kafka 则是 Follower 副本向 Leader 副本发送获取数据的请求进行同步，这种请求与消费者为了读取消息发送的请求是一样的。</p><p>既然存在数据的复制，那肯定会存在副本间数据不一致的情况。Kafka 为了保证数据的可靠性，定义了<strong>同步的副本（in-sync replicas）</strong>，只有数据被写到了所有的同步副本中才能被消费者读到，这个同步的副本组成的集合称为 <strong>ISR</strong> 。</p><p>那么，哪些副本属于 ISR ? 首先 Leader 副本肯定是同步副本，对于 Follower 副本需要满足以下条件：</p><ul><li>与 Zookeeper 保持会话，即在过去 6s（可 <code>zookeeper.session.timeout.ms</code> 配置）内向 Zookeeper 发送过心跳。</li><li>在过去的 10s 内（可通过 <code>replica.lag.time.max.ms</code> 配置）从 Leader 那里获取过消息。</li><li>在过去的 10s 内从 Leader 那里获取过<strong>最新</strong>的消息。光从 Leader 那里获取消息是不够的，它还必须是几乎零延迟的。</li></ul><p>如果跟 Follower 副本不能满足以上任何一点，比如与 Zookeeper 断开连接，或者在 10s 内没有请求获取任何消息，或者获取消息滞后了 10s 以上，那么它就被认为是不同步的。一个不同步的副本通过与 Zookeeper 重新建立连接，并从 Leader 那里获取最新消息，可以重新变成同步的。</p><blockquote><p>如果一个或多个副本在同步和非同步状态之间快速切换，说明集群内部出现了问题，通常是 Java 不恰当的垃圾回收配置导致的。不恰当的垃圾回收配置会造成几秒钟的停顿，从而让 broker 与 Zookeeper 之间断开连接，最后变成不同步的，进而发生状态切换。</p></blockquote><h3 id="分区分配"><a href="#分区分配" class="headerlink" title="分区分配"></a>分区分配</h3><p>现在，我们已经大概了解了 Kafka 的副本机制，那这些副本在 broker 中是怎么保存的呢？</p><p>我们的目标是要均衡 broker 的负载，由于数据的读写都是通过 Leader 副本，所以 Leader 副本需要均衡地分配到不同的 broker 上。这样做的好处不仅是负载均衡了，而且当某一台 broker 挂掉之后不至于所有 Leader 副本都不能进行读写，受影响的只有部分 Leader 副本。</p><p>举个例子，假设我们有 3 个 broker，每个 Topic 有三个分区，每个分区有 3 个副本。</p><p>那么为了实现均衡分配，我们可以先随机选择一个 broker（假设是 0 ），然后使用轮询的方式将分区 Leader 副本分配到 broker 上。于是，分区 0 的 Leader 副本会在 broker 0 上，分区 1 的 Leader 副本会在 broker 1 上，分区 2 的 Leader 副本会在 broker 3 上，以此类推。 </p><p>Leader 副本分配完之后，依次分配 Follower 副本。如果分区 0 的 Leader 在 broker 0 上，那么它的第一个 Follower 副本会在 broker 1 上，第二个 Follower 副本会在 broker 2 上。分区 1 的 Leader 在 broker 1 上，那么它的第一个 Follower 副本在 broker 2 上，第二个 Follower 副本在 broker 0 上，以此类推。</p><p>最终一个 Topic 三个分区，三个副本分配的结果如下图所示：</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/kafka-replica.png" alt="Kafka Replication"></p><p>这也解析了上面说到的为什么 Kafka 不需要读写分离，因为负载已经均衡到每一个 broker 上了。</p><h3 id="首选-Leader"><a href="#首选-Leader" class="headerlink" title="首选 Leader"></a>首选 Leader</h3><p>我们试想一下上图的情景，假设 broker 2 挂了，那么 Leader 2 也就挂了，此时 Kafka 会从 broker 0 或者 broker 1 中选择一个 Follower 2 提升为 Leader。当 broker 2 再次启动起来之后，原来的 Leader 2 将变成 Follower 2，此时 broker 2 没有 Leader 副本了， broker 0 或者 broker 1 将会有两个 Leader，这样就会导致负载不均衡了。</p><p>为了解决上面的问题， Kafka 中引入<strong>首选 Leader</strong>的概念，也就是优先会成为 Leader 的副本。默认情况下，Kafka 的 <code>auto.leader.rebalance. enable</code> 被设为 true，它会检查首选 Leader 是不是当前 Leader ，如果不是，并且该副本是同步的，那么就会触发 Leader 选举，让首选 Leader 成为当前 Leader，让负载变得更均衡。</p><p><code>auto.leader.rebalance. enable</code> 参数还需要配合以下两个参数一起使用：</p><ul><li><code>leader.imbalance.check.interval.seconds</code>: 检查负载不均衡的时间间隔，默认 300 秒</li><li><code>leader.imbalance.per.broker.percentage</code>: 每个 broker 上 Leader 不均衡的比例超过多少才会触发选举首选 Leader，默认 10%</li></ul><p>那么，哪个是首选 Leader？Kafka 在创建 Topic 时选定的 Leader 就是分区的首选 Leader，我们可以使用 <code>kafka.topics.sh</code> 工具查看副本和分区的详细信息，清单里的第一个副本一般就是首选 Leader。不管当前 Leader 是哪一个副本，都不会改变这个事实，即使使用副本分配工具将副本重新分配给其他 broker。如果我们需要手动进行副本分配，第一个指定的副本就是首选 Leader，所以要确保首选 Leader 被均衡地分配到各个 broker 上。</p><h3 id="选举"><a href="#选举" class="headerlink" title="选举"></a>选举</h3><p>前面提到，当一个分区的 Leader 挂了之后，会选举一个 Follower 提升为 Leader。当然，Kafka 会优先从同步的副本中选择一个提升为 Leader ，这样就不会丢失数据，这个选举就是 “完全”的。但如果在首领不可用时其他副本都是不同步的，我们该怎么办?</p><p>这个时候，我们需要根据业务情况作出一个权衡，因为：</p><ul><li>如果不同步的副本不能被提升为新 Leader ，那么分区在旧 Leader (最后一个同步副本)恢复之前是不可用的。</li><li>如果不同步的副本可以被提升为新 Leader，那么在这个副本变为不同步之后写入旧  Leader 的消息会全部丢失，导致数据不一致。</li></ul><p>也就是说，我们需要在可用性和一致性之间做一个决策。Kafka 中提供了 <code>unclean.leader.election.enable</code> 参数让我们配置，如果把 <code>unclean.leader.election.enable</code> 设为 true，就是允许不同步的副本成为 Leader (也就是“不完全的选举”)，那么我们将面临丢失消息的风险。如果把这个参数设为 false， 就要等待原先的 Leader 重新上线，从而降低了可用性。</p><h3 id="最小同步副本"><a href="#最小同步副本" class="headerlink" title="最小同步副本"></a>最小同步副本</h3><p>根据 Kafka 对可靠性保证的定义，消息只有在被写入到所有同步副本之后才被认为是已提交的，才能被客户端消费。但如果这里的“所有副本”只包含一个同步副本，那么在这个副本变为不可用时，数据就会丢失。</p><p>Kafka 提供了一个最小同步副本参数 <code>min.insync.replicas</code> 让我们配置。如果要确保已提交的数据被写入不止一个副本，就需要把最少同步副本数量设置为大一点的值。对于一个包含 3 个副本的 Topic ，如果 <code>min.insync.replicas</code> 被设为 2，那么至少要存在两个同步副本才能向分区写入数据。如果只有一个副本，那么 broker 就会停止接受生产者的请求，尝试发送数据的生产者会收到 <code>NotEnoughReplicasException</code> 异常。消费者仍然可以继续读取已有的数据。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Kafka 副本分为 Leader 和 Follower</li><li>同步副本集合称为 ISR ，只有被写进所有同步副本的消息才能被消费者读取到</li><li>分区需要均衡地分配到每个 broker </li><li>Kafka 的首选 Leader 是为了解决 Leader 迁移之后引发的负载不均衡问题</li><li>默认可以进行“不完全”选举，需要根据业务情况在可用性和一致性之间进行权衡</li><li>当同步副本数量小于最小同步副本时，生产者写入数据会收到异常信息，消费者仍可读取已有数据</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前研究了一下 Redis 的主从复制，现在对比学习一下 Kafka 的数据复制。&lt;/p&gt;
&lt;h3 id=&quot;副本&quot;&gt;&lt;a href=&quot;#副本&quot; class=&quot;headerlink&quot; title=&quot;副本&quot;&gt;&lt;/a&gt;副本&lt;/h3&gt;&lt;p&gt;Kafka 用 Topic 来组织数据，每
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Redis Sentinel Client</title>
    <link href="http://ideajava.com/2020/01/20/sentinel-client/"/>
    <id>http://ideajava.com/2020/01/20/sentinel-client/</id>
    <published>2020-01-20T07:00:36.000Z</published>
    <updated>2020-01-21T06:49:06.712Z</updated>
    
    <content type="html"><![CDATA[<p>前几篇文章研究了 Redis 服务端的高可用方案 Sentinel 模式，今天我们来研究一下 Sentinel 的客户端工作原理。</p><p>本文以 SpringBoot 和 Jedis 为例，先介绍客户端的使用，然后分析工作流程。在开始阅读文章之前，请大家思考一个问题，Sentinel 模式下，如果主节点发生了切换，客户端是怎么知道的？</p><h3 id="SpringBoot-Redis-配置"><a href="#SpringBoot-Redis-配置" class="headerlink" title="SpringBoot Redis 配置"></a>SpringBoot Redis 配置</h3><p>SpringBoot 中使用 Redis 很简单，只需要引入相应的依赖和配置相关参数即可：</p><ul><li>依赖引入</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;redis.clients&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;jedis&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><ul><li>配置</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  redis:</span><br><span class="line">database: 0</span><br><span class="line">password: $&#123;REDIS_PWD&#125;</span><br><span class="line">jedis: </span><br><span class="line">  pool:</span><br><span class="line">    max-active: 8  </span><br><span class="line">    max-wait: -1    </span><br><span class="line">    max-idle: 8      </span><br><span class="line">    min-idle: 2       </span><br><span class="line">sentinel:</span><br><span class="line">  master: mymaster</span><br><span class="line">  nodes:</span><br><span class="line">  - sentinel-0:26379</span><br><span class="line">  - sentinel-1:26379</span><br><span class="line">  - sentinel-2:26379</span><br></pre></td></tr></table></figure><p>上面是一个示例配置，在使用 Sentinel 模式之后，我们并不是直接配置 Redis 服务器主节点的 host 和 port 了，因为主节点是有可能切换的，我们并不能写死一个主服务器的 host 和 port。取而代之，我们配置的是 Sentinel 节点的 host 和 port，所以我们可以大胆猜测在使用 Sentinel 之后客户端的工作流程：</p><ol><li>通过任一可用的 Sentinel 节点获取 Redis 主服务器 host 和 port</li><li>建立与 Redis 主服务器的连接池</li><li>通过连接池获取一个连接与主服务进行读写等操作</li><li>如果发生了主服务器切换，重建连接池的连接到新的主服务器</li></ol><p>又回到了文章开头的问题，客户端怎么知道主服务器发生了切换呢？以我浅薄的工作经验再次大胆猜测：</p><ol><li>不断地去问 Sentinel 主服务器有没有变啊？</li><li>要不 Sentinel 直接告诉我？Redis 不是有个发布订阅功能吗。</li></ol><p>嗯，上面的工作流程和切换问题都是我瞎猜的，带着问题去看源码是我一贯的作风。</p><h3 id="SpringBoot-Redis-启动流程"><a href="#SpringBoot-Redis-启动流程" class="headerlink" title="SpringBoot Redis 启动流程"></a>SpringBoot Redis 启动流程</h3><p>如果从使用的源头查看，比如 <code>redisTemplate.opsForValue().set(&quot;key1&quot;,&quot;value1&quot;);</code> ，跟踪其实现方法，我们会发现是通过一个 <code>RedisConnection</code> 进行操作。而 RedisConnection 是通过 <code>RedisConnectionFactory</code> 获取的，所以 RedisConnectionFactory 的初始化对于我们了解整个流程至关重要。</p><p>我们回头看一下，在 SpringBoot 中使用 Redis 只引入了依赖和配置参数，并没有编写任何一行代码，这其实是 SpringBoot 的 AutoConfiguration 帮我们“偷偷”干了活。由于篇幅原因这部分就不展开了，我画了一个大致的关系图：</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/springboot-autoconfiguration.png" alt="SpringBoot AutoConfigruation"></p><p>由上图可知，<code>RedisAutoConfiguration</code> 帮我们实现了 Redis 的自动装配，我们看下 RedisAutoConfiguration 顶部的注解内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@Configuration(proxyBeanMethods = false)</span><br><span class="line">@ConditionalOnClass(RedisOperations.class)</span><br><span class="line">@EnableConfigurationProperties(RedisProperties.class)</span><br><span class="line">@Import(&#123; LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class &#125;)</span><br><span class="line">public class RedisAutoConfiguration &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Import 了另外两个配置类，由于我们的例子引入的是 Jedis 客户端，所以主要是看 JedisConnectionConfiguration 干了那些活。</p><h3 id="JedisConnectionConfiguration"><a href="#JedisConnectionConfiguration" class="headerlink" title="JedisConnectionConfiguration"></a>JedisConnectionConfiguration</h3><p>我们再来看一下 <code>JedisConnectionConfiguration</code> 的部分代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@Configuration(proxyBeanMethods = false)</span><br><span class="line">@ConditionalOnClass(&#123; GenericObjectPool.class, JedisConnection.class, Jedis.class &#125;)</span><br><span class="line">class JedisConnectionConfiguration extends RedisConnectionConfiguration &#123;</span><br><span class="line">@Bean</span><br><span class="line">@ConditionalOnMissingBean(RedisConnectionFactory.class)</span><br><span class="line">JedisConnectionFactory redisConnectionFactory(</span><br><span class="line">ObjectProvider&lt;JedisClientConfigurationBuilderCustomizer&gt; builderCustomizers) throws UnknownHostException &#123;</span><br><span class="line">return createJedisConnectionFactory(builderCustomizers);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private JedisConnectionFactory createJedisConnectionFactory(</span><br><span class="line">ObjectProvider&lt;JedisClientConfigurationBuilderCustomizer&gt; builderCustomizers) &#123;</span><br><span class="line">JedisClientConfiguration clientConfiguration = getJedisClientConfiguration(builderCustomizers);</span><br><span class="line">if (getSentinelConfig() != null) &#123;</span><br><span class="line">return new JedisConnectionFactory(getSentinelConfig(), clientConfiguration);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这里，我们就知道核心的入口了，这里创建了一个 JedisConnectionFactory ，顾名思义，我们需要用到的 Redis 连接就是从这个工厂类获取的。在第二个方法中，有个 <code>getSentinelConfig()</code> 方法，一看就知道这是获取我们文章开头的 Sentinel 配置。</p><p>所以，JedisConnectionConfiguration 的作用大致就是读取我们的文章开头的配置，然后初始化一个 <code>JedisConnectionFactory</code> 的 Bean。</p><h3 id="JedisConnectionFactory"><a href="#JedisConnectionFactory" class="headerlink" title="JedisConnectionFactory"></a>JedisConnectionFactory</h3><p>我们再来看一下创建 JedisConnectionFactory 的时候做了哪些事情。</p><p>从上面的方法看出，JedisConnectionFactory 是通过 <code>new JedisConnectionFactory(getSentinelConfig(), clientConfiguration)</code> 创建的，我们看下这个构造方法的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public JedisConnectionFactory(RedisSentinelConfiguration sentinelConfig, JedisClientConfiguration clientConfig) &#123;</span><br><span class="line"></span><br><span class="line">this(clientConfig);</span><br><span class="line"></span><br><span class="line">Assert.notNull(sentinelConfig, &quot;RedisSentinelConfiguration must not be null!&quot;);</span><br><span class="line"></span><br><span class="line">this.configuration = sentinelConfig;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private JedisConnectionFactory(JedisClientConfiguration clientConfig) &#123;</span><br><span class="line"></span><br><span class="line">Assert.notNull(clientConfig, &quot;JedisClientConfiguration must not be null!&quot;);</span><br><span class="line"></span><br><span class="line">this.clientConfiguration = clientConfig;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>似乎没干啥，只是简单地设置了 clientConfiguration 和 configuration 两个属性。到这里思路好像断了，难道设置两个属性就能干活了吗？前面推论的连接池呢，按理说 JedisConnectionFactory 应该会创建一个连建池，然后获取连接的时候从连接池取一个可用连接出来，但现在并没有看到这样的代码。</p><p>此时，只能继续翻看一下 JedisConnectionFactory 还有哪些方法，果然有所发现，有一个属性设置之后的后置方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public void afterPropertiesSet() &#123;</span><br><span class="line">...</span><br><span class="line">if (getUsePool() &amp;&amp; !isRedisClusterAware()) &#123;</span><br><span class="line">this.pool = createPool();</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Pool&lt;Jedis&gt; createPool() &#123;</span><br><span class="line"></span><br><span class="line">if (isRedisSentinelAware()) &#123;</span><br><span class="line">return createRedisSentinelPool((RedisSentinelConfiguration) this.configuration);</span><br><span class="line">&#125;</span><br><span class="line">return createRedisPool();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected Pool&lt;Jedis&gt; createRedisSentinelPool(RedisSentinelConfiguration config) &#123;</span><br><span class="line"></span><br><span class="line">GenericObjectPoolConfig poolConfig = getPoolConfig() != null ? getPoolConfig() : new JedisPoolConfig();</span><br><span class="line">return new JedisSentinelPool(config.getMaster().getName(), convertToJedisSentinelSet(config.getSentinels()),</span><br><span class="line">poolConfig, getConnectTimeout(), getReadTimeout(), getPassword(), getDatabase(), getClientName());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，当我们配置了 Sentinel 信息之后，会创建一个 <code>JedisSentinelPool</code>。</p><h3 id="JedisSentinelPool"><a href="#JedisSentinelPool" class="headerlink" title="JedisSentinelPool"></a>JedisSentinelPool</h3><p>我们再来看下创建 JedisSentinelPool 的构造方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public JedisSentinelPool(String masterName, Set&lt;String&gt; sentinels,</span><br><span class="line">     final GenericObjectPoolConfig poolConfig, final int connectionTimeout, final int soTimeout,</span><br><span class="line">     final String password, final int database, final String clientName) &#123;</span><br><span class="line">   ...</span><br><span class="line">   HostAndPort master = initSentinels(sentinels, masterName);</span><br><span class="line">   initPool(master);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>到这里，大概就可以证实我们前两点猜想：</p><ol><li>通过 Sentinel 获取主节点的 host 和 port</li><li>初始化到主节点的连接池</li></ol><p>我们分别看下这两个方法是怎么实现的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> private HostAndPort initSentinels(Set&lt;String&gt; sentinels, final String masterName) &#123;</span><br><span class="line"></span><br><span class="line">   HostAndPort master = null;</span><br><span class="line">...</span><br><span class="line">   for (String sentinel : sentinels) &#123;</span><br><span class="line">       ...</span><br><span class="line">       List&lt;String&gt; masterAddr = jedis.sentinelGetMasterAddrByName(masterName);</span><br><span class="line">...</span><br><span class="line">       if (masterAddr == null || masterAddr.size() != 2) &#123;</span><br><span class="line">         log.warn(&quot;Can not get master addr, master name: &#123;&#125;. Sentinel: &#123;&#125;&quot;, masterName, hap);</span><br><span class="line">         continue;</span><br><span class="line">       &#125;</span><br><span class="line">       ...</span><br><span class="line">       master = toHostAndPort(masterAddr);</span><br><span class="line">       break;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">   for (String sentinel : sentinels) &#123;</span><br><span class="line">     final HostAndPort hap = HostAndPort.parseString(sentinel);</span><br><span class="line">     MasterListener masterListener = new MasterListener(masterName, hap.getHost(), hap.getPort());</span><br><span class="line">     masterListener.setDaemon(true);</span><br><span class="line">     masterListeners.add(masterListener);</span><br><span class="line">     masterListener.start();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   return master;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>原来这里不仅仅从可用的 Sentinel 中获取主节点的 host 和 port，还为每个 Sentinel 注册了一个 <code>MasterListener</code> 的监听器，看情况就是这里告诉我们主节点变更了。为了不打断主分析流程，我们先不看这个监听器，接着看获取到主节点信息之后怎么初始化连接池：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private void initPool(HostAndPort master) &#123;</span><br><span class="line">  synchronized(initPoolLock)&#123;</span><br><span class="line">    if (!master.equals(currentHostMaster)) &#123;</span><br><span class="line">      currentHostMaster = master;</span><br><span class="line">      if (factory == null) &#123;</span><br><span class="line">        factory = new JedisFactory(master.getHost(), master.getPort(), connectionTimeout,</span><br><span class="line">            soTimeout, password, database, clientName);</span><br><span class="line">        initPool(poolConfig, factory);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        factory.setHostAndPort(currentHostMaster);</span><br><span class="line">        internalPool.clear();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> public void initPool(final GenericObjectPoolConfig poolConfig, PooledObjectFactory&lt;T&gt; factory) &#123;</span><br><span class="line">  if (this.internalPool != null) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      closeInternalPool();</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  this.internalPool = new GenericObjectPool&lt;T&gt;(factory, poolConfig);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>嗯，判断入参的主节点是不是当前主节点，如果不是的话就会初始化连接池，第一次调用该方法时，当前主节点为空也会进行初始化（重写了 equals 方法），连接池使用的是 apache commons pool2。</p><p>获取主节点信息和初始化连接池我们已经了解了，接下来看看 initSentinel 时注册的 <code>MasterListener</code> 干了啥。</p><h3 id="MasterListener"><a href="#MasterListener" class="headerlink" title="MasterListener"></a>MasterListener</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">protected class MasterListener extends Thread &#123;</span><br><span class="line">  public void run() &#123;</span><br><span class="line"></span><br><span class="line">     running.set(true);</span><br><span class="line"></span><br><span class="line">     while (running.get()) &#123;</span><br><span class="line"></span><br><span class="line">       j = new Jedis(host, port);</span><br><span class="line">         ...</span><br><span class="line">         List&lt;String&gt; masterAddr = j.sentinelGetMasterAddrByName(masterName);  </span><br><span class="line">         if (masterAddr == null || masterAddr.size() != 2) &#123;</span><br><span class="line">           log.warn(&quot;Can not get master addr, master name: &#123;&#125;. Sentinel: &#123;&#125;：&#123;&#125;.&quot;,masterName,host,port);</span><br><span class="line">         &#125;else&#123;</span><br><span class="line">             initPool(toHostAndPort(masterAddr)); </span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         j.subscribe(new JedisPubSub() &#123;</span><br><span class="line">           @Override</span><br><span class="line">           public void onMessage(String channel, String message) &#123;</span><br><span class="line"></span><br><span class="line">             String[] switchMasterMsg = message.split(&quot; &quot;);</span><br><span class="line"></span><br><span class="line">             if (switchMasterMsg.length &gt; 3) &#123;</span><br><span class="line"></span><br><span class="line">               if (masterName.equals(switchMasterMsg[0])) &#123;</span><br><span class="line">                 initPool(toHostAndPort(Arrays.asList(switchMasterMsg[3], switchMasterMsg[4])));</span><br><span class="line">               &#125; else &#123;</span><br><span class="line">  ...</span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">             &#125; else &#123;</span><br><span class="line">              ...</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;</span><br><span class="line">         &#125;, &quot;+switch-master&quot;);</span><br><span class="line"></span><br><span class="line">...     </span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到这里，文章开头的问题终于真相大白，原来是有单独的线程监听每个 Sentinel 的 <code>+switch-master</code> 主题，当 Sentinel 发生主节点切换时会通过 <code>+switch-master</code> 主题发布消息，然后 <code>MasterListener</code> 线程监听到之后重新初始化连接池。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们回顾一下整个流程</p><ol><li>SpringBoot 通过 AutoConfiguration 自动装配创建 JedisConnectionFactory</li><li>JedisConnectionFactory 根据配置信息创建 JedisSentinelPool</li><li>JedisSentinelPool 通过 Sentinel 获取主节点信息，然后初始化连接池</li><li>JedisSentinelPool 在 initSentinel 时除了获取主节点信息，还会为每个 Sentinel 创建一个 MasterListener 监听器</li><li>MasterListener 通过监听 Sentinel 的 <code>+switch-master</code> 主题观测主节点的切换，然后创建连接池</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前几篇文章研究了 Redis 服务端的高可用方案 Sentinel 模式，今天我们来研究一下 Sentinel 的客户端工作原理。&lt;/p&gt;
&lt;p&gt;本文以 SpringBoot 和 Jedis 为例，先介绍客户端的使用，然后分析工作流程。在开始阅读文章之前，请大家思考一个问题
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MQTT QoS</title>
    <link href="http://ideajava.com/2020/01/14/mqtt-qos/"/>
    <id>http://ideajava.com/2020/01/14/mqtt-qos/</id>
    <published>2020-01-14T11:56:48.000Z</published>
    <updated>2020-01-15T07:37:09.077Z</updated>
    
    <content type="html"><![CDATA[<p>最近做了个物联网的项目，用到了 MQTT，里面有个 QoS 的概念理解起来可能会有一点困扰，今天花了点时间梳理了一下其中的逻辑，写篇文章记录一下。</p><h3 id="什么是-QoS"><a href="#什么是-QoS" class="headerlink" title="什么是 QoS ?"></a>什么是 QoS ?</h3><p>QoS 的全称是 quality of service。描述的是消息发送方和消息接收方之间消息传递的保证级别。MQTT 中有 3 个 QoS 级别：</p><ul><li>At most once (0)</li><li>At least once (1)</li><li>Exactly once (2)</li></ul><p>MQTT 发布消息 QoS 保证不是端到端的，是客户端与服务器之间的。订阅者收到 MQTT 消息的 QoS 级别，最终取决于发布消息的 QoS 和主题订阅的 QoS。</p><table><thead><tr><th style="text-align:center">发布消息的QoS</th><th style="text-align:center">主题订阅的QoS</th><th style="text-align:center">接收消息的QoS</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">2</td></tr></tbody></table><h3 id="QoS-0-at-most-once"><a href="#QoS-0-at-most-once" class="headerlink" title="QoS 0 - at most once"></a>QoS 0 - at most once</h3><p>最小 QoS 级别为 0 。此服务级别保证了 <em>尽力而为</em> 的投递，不保证一定投递成功。接收方不会回复确认消息，发送方也不存储和重新发送消息。</p><table><thead><tr><th style="text-align:center">发送方</th><th style="text-align:center">报文流向</th><th style="text-align:center">接受方</th></tr></thead><tbody><tr><td style="text-align:center">PUBLISH QoS = 0, DUP = 0</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">—&gt;</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">接收消息（可能不会收到）并处理</td></tr></tbody></table><h3 id="QoS-1-at-least-once"><a href="#QoS-1-at-least-once" class="headerlink" title="QoS 1 - at least once"></a>QoS 1 - at least once</h3><table><thead><tr><th style="text-align:center">发送方</th><th style="text-align:center">报文流向</th><th style="text-align:center">接受方</th></tr></thead><tbody><tr><td style="text-align:center">存储消息</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">发送 PUBLISH QoS1, DUP = 0，带有 Packetld</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">—&gt;</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">接收消息并处理</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">发送带有 Packetld 和 PUBACK 确认报文</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">&lt;—</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">丢弃消息</td><td style="text-align:center"></td></tr></tbody></table><p>只要发送方没有收到 <code>PUBACK</code> 回复报文，消息还会被重新发送，所有 QoS 1 可能会出现消息重复的问题。</p><p>需要注意的是，接收方在接收 QoS 1 消息时并不会幂等处理，也就是说消息重发也认为是一条新的消息（至于 DUP 标志，协议里有说 “DUP = 1“ 时，也不能假设认为之前收到过该报文）。</p><h3 id="QoS-2-exactly-once"><a href="#QoS-2-exactly-once" class="headerlink" title="QoS 2 - exactly once"></a>QoS 2 - exactly once</h3><table><thead><tr><th style="text-align:center">发送方</th><th style="text-align:center">报文流向</th><th style="text-align:center">接受方</th></tr></thead><tbody><tr><td style="text-align:center">存储消息</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">发送 PUBLISH QoS2, DUP = 0，带有 PacketId</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">—&gt;</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">存储 PacketId 和消息</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">发布带有 Packetld 和 Reason Code 的 PUBREC 报文</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">&lt;—</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">丢弃存储的消息，存储接收到的带有相同 PacketId 的 PUBREC 报文</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">发送 PUBREL 报文</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">—&gt;</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">处理消息，然后丢弃 PacketId</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">发送带有 PacketId 的 PUBCOMP 报文</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">&lt;—</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">丢弃存储的状态</td><td style="text-align:center"></td></tr></tbody></table><p>QoS 2 应该是最难理解的了，涉及了四次交互，如下图所示</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/qos2.png" alt="QoS 2"></p><p>简单来说，QoS 2 有点像分布式事务中 <strong>两阶段提交</strong> 的解决方案，第一阶段的请求响应只是将消息保存起来的，但还不能作为正式的消息去处理，相当于一个 <strong>半提交</strong> 的状态，第二阶段就是确认消息提交，可以进行处理了。</p><p>QoS 2 保证只到达了一次是因为在这四次交互之中根据 PacketId 做了幂等处理，但是在处理消息之后 PacketId 会被丢弃，也就是说在发起新一轮的四次交互 PacketId 是可以重用的。至于 QoS 2 怎么实现幂等处理，大家找个 MQTT 客户端的源码看一下。</p><p>对于 QoS 2 如果还有疑问的可以继续看下下面两篇文章：</p><ul><li><a href="http://www.steves-internet-guide.com/understanding-mqtt-qos-2/" target="_blank" rel="noopener">Understanding MQTT QOS Levels- Part 2</a></li><li><a href="https://www.zhihu.com/question/54000916" target="_blank" rel="noopener">MQTT协议中QoS2为什么要四次包交互？</a></li></ul><p>当然，源码是最好的老师。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近做了个物联网的项目，用到了 MQTT，里面有个 QoS 的概念理解起来可能会有一点困扰，今天花了点时间梳理了一下其中的逻辑，写篇文章记录一下。&lt;/p&gt;
&lt;h3 id=&quot;什么是-QoS&quot;&gt;&lt;a href=&quot;#什么是-QoS&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Redis 哨兵模式（二）</title>
    <link href="http://ideajava.com/2020/01/10/redis-sentinel-2/"/>
    <id>http://ideajava.com/2020/01/10/redis-sentinel-2/</id>
    <published>2020-01-10T08:50:38.000Z</published>
    <updated>2020-01-13T08:50:03.988Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中学习了 Sentinel 基本的数据结构和消息通信，现在我们来看一下监控和故障转移。</p><h3 id="检测主观下线"><a href="#检测主观下线" class="headerlink" title="检测主观下线"></a>检测主观下线</h3><p>众所周知，检测服务是否可用的常用方式就是通过发起一个请求，等待服务的响应，如果服务在某个规定的时间点内都没有响应，那么我们就会认为服务挂掉了。</p><p>Sentinel 检测主观下线也是采用类似的方式，在默认的情况下，Sentinel 会以每秒一次的频率向所有与它创建了命令连接的实例（包括主服务器，从服务器和其他 Sentinel）发送 PING 命令，并通过响应信息来判断实例是否在线。</p><ul><li>有效响应：实例返回 +PONG, -LOADING, -MASTERDOWN 三种其中一种</li><li>无效响应：返回以上三种之外的的其他响应或者是没有响应</li></ul><p>Sentinel 配置文件中的 <code>down-after-milliseconds</code> 属性用于判断实例进入主观下线所需要的时间：即在 <code>down-after-milliseconds</code> 配置的毫秒数内，连续向 Sentinel 返回无效响应，则认为实例进入主观下线状态，此时会更新数据结构中的 flags 属性，打开 <code>SRI_S_DOWN</code> 标识。</p><p>比如，主服务器在 <code>down-after-milliseconds</code> 时间范围内没有返回有效响应，则数据结构会更新如下：</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/sri_s_down.png" alt="Sentinel network"></p><p><strong>注意：</strong></p><ul><li><code>down-after-milliseconds</code> 不仅用于判断主服务器主观下线，还用于从服务器和其他 Sentinel</li><li>每个 Sentinel 配置的 <code>down-after-milliseconds</code> 时间有可能不同，如果配置不同，会出现 Sentinel1 认为已经主观下线，而 Sentinel2 不这样认为</li></ul><h3 id="检测客观下线"><a href="#检测客观下线" class="headerlink" title="检测客观下线"></a>检测客观下线</h3><p>当 Sentinel 将一个主服务器判断为下线之后，为了确认该主服务是否真的下线，它会向同样监视这一主服务器的其他 Sentinel 进行询问，看它们是否也认为主服务已经进入了下线状态（可以是主观下线或者客观下线）。当 Sentinel 从其他 Sentinel 那里接受到足够数量的已下线判断，Sentinel 就会将主服务器判断为客观下线，并对其执行故障转移操作。</p><h4 id="发送-Sentinel-is-master-down-by-addr-命令"><a href="#发送-Sentinel-is-master-down-by-addr-命令" class="headerlink" title="发送 Sentinel is-master-down-by-addr 命令"></a>发送 Sentinel is-master-down-by-addr 命令</h4><p>Sentinel 通过使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sentinel is-master-down-by-addr &lt;ip&gt; &lt;port&gt; &lt;current_epoch&gt; &lt;runid&gt;</span><br></pre></td></tr></table></figure><p>命令询问其他 Sentinel 是否判断主服务已经下线，各参数意义如下：</p><ul><li>ip: 被 Sentinel 判断为主观下线的主服务器 IP 地址</li><li>port: 被 Sentinel 判断为主观下线的主服务器 PORT 地址</li><li>current_epoch: Sentinel 当前的配置纪元，用于选举 Leader Sentinel</li><li>run_id: 可以是 <em> 或者 Sentinel 的 run_id：</em> 代表仅用于检测主服务器的客观下线状态，而 run_id 则用于选举 Leader Sentinel</li></ul><h4 id="接收-Sentinel-is-master-down-by-addr-命令"><a href="#接收-Sentinel-is-master-down-by-addr-命令" class="headerlink" title="接收 Sentinel is-master-down-by-addr 命令"></a>接收 Sentinel is-master-down-by-addr 命令</h4><p>当一个 Sentinel 接收到另一个 Sentinel 的 <code>is-master-down-by-addr</code> 命令之后，会根据参数来检测主服务是否已经下线，然后返回一条包含以下三个参数的回复：</p><ul><li>down_status: 1 代表主服务已下线，0 代表未下线</li><li>leader_runid: 可以是 * 或者该响应 Sentinel 判断出的局部 Leader Sentinel run_id</li><li>leader_epoch: 仅在 leader_runid 不为 <em> 时有效，leader_runid 为 </em> 时总是 0</li></ul><h4 id="接收-Sentinel-is-master-down-by-addr-命令的回复"><a href="#接收-Sentinel-is-master-down-by-addr-命令的回复" class="headerlink" title="接收 Sentinel is-master-down-by-addr 命令的回复"></a>接收 Sentinel is-master-down-by-addr 命令的回复</h4><p>Sentinel 根据回复统计其他 Sentinel 判断主服务已下线的数量，当这一数量达到配置指定的判断客观下线所需的数量时，Sentinel 就会将主服务 sentinelRedisInstance 数据结构中的 flags 属性 SRI_O_DOWN 标识打开，表示主服务已经进入客观下线状态。</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/sri_o_down.png" alt="Sentinel network"></p><p><strong>注意：</strong> 不同的 Sentinel 配置判断客观下线的数量可能存在不同</p><h3 id="选举-Leader-Sentinel"><a href="#选举-Leader-Sentinel" class="headerlink" title="选举 Leader Sentinel"></a>选举 Leader Sentinel</h3><p>当一个主服务器被判断为客观下线时，监视这个下线主服务的各个 Sentinel 会进行协商，选举出一个 Leader Sentinel，并由 Leader Sentinel 对下线的主服务器执行故障转移。</p><p>选举规则和方法：</p><ul><li>所有在线的 Sentinel 都有被选举为 Leader 的资格</li><li>每次选举之后，不论选举是否成功，所有 Sentinel 的 epoch 都会自增一次</li><li>在一个配置纪元里面，所有 Sentinel 都有一次将某个 Sentinel 设置为局部 Leader Sentinel 的机会，并且设置之后再这个纪元之内就不能改变</li><li>每个发现主服务进入客观下线的 Sentinel 都会要求其他 Sentinel 将自己设置为局部 Leader Sentinel</li><li>当 Sentinel 向其他 Sentinel 发送 <code>is-master-down-by-addr</code> 命令中参数设置了自己的 run_id 就是让其他 Sentinel 选举自己为 Leader</li><li>Sentinel 设置局部 Leader 的规则是先到先得，后到的会被拒绝</li><li>Sentinel 会根据 <code>is-master-down-by-addr</code> 命令的回复取出 epoch 看是否跟自己的一致，如果一致再取出 leader_runid 看是否跟自己一直，如果一致则表示选举了自己为局部 Leader</li><li>如果某个 Sentinel 有超过半数的 Sentinel 设置自己为局部 Leader，那么这个 Sentinel 成为真正的 Leader</li><li>如果在规定的时间内没有选举出一个 Leader ，那么在一段时间之后再次选举，知道选举出一个 Leader 为止</li></ul><p>有熟悉的读者可能已经看出，这就是 Raft 选举算法。</p><h3 id="故障转移"><a href="#故障转移" class="headerlink" title="故障转移"></a>故障转移</h3><p>选举出 Leader 之后，Leader 将会对已下线的主服务器执行故障转移操作，该操作包含三个步骤：</p><ol><li>挑选一个从服务器作为新的主服务器</li><li>让其他从服务器改为复制新的主服务器</li><li>将已下线的主服务设置为新的主服务器的从服务器</li></ol><h4 id="选出新的主服务器"><a href="#选出新的主服务器" class="headerlink" title="选出新的主服务器"></a>选出新的主服务器</h4><p>故障转移的第一步就是挑选出一个状态良好，数据完整的从服务器，然后向其发送 SLAVEOF no one 命令，将这个从服务器转换为主服务器。</p><p>Leader 会将已下线主服务器的所有从服务器保存到一个列表里面，然后按照以下规则，一项项地对列表进行过滤：</p><ol><li>删除列表中处于下线或者断线状态的从服务器</li><li>删除列表中最近五秒没有回复过 Leader Sentinel 的 INFO 命令的从服务器</li><li>删除所有与已下线主服务器连接断开超过 down-after-milliseconds * 10 毫秒的从服务器</li></ol><p>之后，Leader 将根据从服务器的优先级，对列表中剩余的从服务器进行排序，选出优先级最高的从服务器。如果有相同最高优先级的，则选出偏移量最大的。如果偏移量也相同，则根据 run_id 排序选出最小的从服务器。</p><p>在向挑选出的从服务器发送 <code>SLAVEOF no one</code> 命令之后，Leader Sentinel 会以每秒一次的频率（平时是每十秒一次）向被升级的从服务发送 INFO 命令，并观察回复中的 role 信息。当被升级服务器的 role 从原来的 slave 变为 master 时，Leader 就知道被选中的从服务器已经升级为主服务器了。</p><h4 id="修改从服务器的复制目标"><a href="#修改从服务器的复制目标" class="headerlink" title="修改从服务器的复制目标"></a>修改从服务器的复制目标</h4><p>选举出新的主服务器之后，下一步就是向其他从服务器发送 SLAVEOF 命令，让它们复制新的主服务器。</p><h4 id="将旧的主服务器变为从服务器"><a href="#将旧的主服务器变为从服务器" class="headerlink" title="将旧的主服务器变为从服务器"></a>将旧的主服务器变为从服务器</h4><p>因为旧的服务器已经下线，所以这个设置是保存在旧的主服务器 redisSentinelInstance 数据结构中。当旧的主服务器重新上线时，Sentinel 就会向它发送 SLAVEOF 命令，让它成为新的主服务器的从服务器。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>Sentinel 通过每秒一次的 PING 命令来判断服务器是否主观下线</li><li>对于主观下线的主服务器，Sentinel 通过 <code>is-master-down-by-addr</code> 命令询问其他 Sentinel 是否也判断为主观下线</li><li>当有足够多的 Sentinel 判断主服务为主观下线则会设置为客观下线</li><li>当主服务器客观下线之后，Sentinel 会通过 Raft 算法选举出一个 Leader Sentinel 进行故障转移</li><li>故障转移三个步骤：选举新主，其他从服务器复制新主，旧服务器设置为新主的 Slave</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一篇文章中学习了 Sentinel 基本的数据结构和消息通信，现在我们来看一下监控和故障转移。&lt;/p&gt;
&lt;h3 id=&quot;检测主观下线&quot;&gt;&lt;a href=&quot;#检测主观下线&quot; class=&quot;headerlink&quot; title=&quot;检测主观下线&quot;&gt;&lt;/a&gt;检测主观下线&lt;/h3&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Redis 哨兵模式（一）</title>
    <link href="http://ideajava.com/2020/01/06/redis-sentinel-1/"/>
    <id>http://ideajava.com/2020/01/06/redis-sentinel-1/</id>
    <published>2020-01-06T11:17:12.000Z</published>
    <updated>2020-01-07T12:25:48.783Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章研究了一下 Redis 的主从复制，如果再进一步思考，假设发生了主从切换的情况，客户端是怎么感知到，从而连接到主服务器进行读写操作呢？</p><p>今天我们要研究的 Sentinel(哨兵) 模式就是 Redis 高可用的一种方案：由一个或多个 Sentinel 实例组成的 Sentinel 系统，可以监控任意多个主服务器，以及这些主服务器下的所有从服务器。当监控到主服务器下线之后，可以自动将某个从服务器升级为新的主服务器。</p><p>由于篇幅原因，今天先来了解一下 Sentinel 的一些数据结构和消息通信的基本知识，有了今天的基础知识之后，下一篇文章再来了解如何检测服务器下线情况和故障转移。</p><h3 id="什么是-Sentinel"><a href="#什么是-Sentinel" class="headerlink" title="什么是 Sentinel"></a>什么是 Sentinel</h3><p>Redis Sentinel 其实就是一个比较特殊的 Redis 服务器，可以通过 <code>redis-sentinel /path/to/your/sentinel.conf</code> 或者 <code>redis-server /path/to/your/sentinel.conf --sentinel</code> 来启动。</p><p>当一个 Sentinel 启动时，它需要执行以下步骤：</p><ol><li>初始化服务器</li><li>将普通 Redis 服务器使用的代码替换成 Sentinel 代码</li><li>初始化 Sentinel 状态</li><li>根据给定的配置文件，初始化 Sentinel 的监控主服务器列表</li><li>创建与主服务器的网络连接</li></ol><p>从上面的步骤我们可以得到的信息：Sentinel 就是一个普通的 Redis 服务器替换了一些达到 Sentinel 功能的代码，根据配置文件初始化并监控主服务器。</p><p>既然要监控 Redis 服务器的情况，那么 Sentinel 肯定有数据结构来保存相关的信息，然后才根据相关信息来做决策。所以，我们要先了解 Sentinel 的数据结构。</p><h3 id="Sentinel-State"><a href="#Sentinel-State" class="headerlink" title="Sentinel State"></a>Sentinel State</h3><p>Sentinel 用一个叫做 sentinelState 的数据结构保存了服务器中所有和 Sentinel 功能有关的状态（服务器的一般状态仍然由 redisServer 数据结构来保存，因为 Sentinel 本质也是一个 Redis 服务器）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">struct sentinelState &#123;</span><br><span class="line">// 当前纪元，用于实现故障转移</span><br><span class="line">uint64_t current_epoch;</span><br><span class="line"></span><br><span class="line">// 保存了所以被这个 sentinel 监控的主服务器</span><br><span class="line">// 字典的 key 是主服务器的名称，在 sentinel.conf 配置文件中指定</span><br><span class="line">// 字典的值是一个指向 sentinelRedisInstance 结构的指针</span><br><span class="line">dict *masters;</span><br><span class="line">int tilt;</span><br><span class="line">int running_scripts;</span><br><span class="line">mstime_t tilt_start_time;</span><br><span class="line">mstime_t previous_time;</span><br><span class="line">list *scripts_queue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>本次我们的主题只需要关注有注释的两个属性，其中重点就是 masters 属性，这里记录了 Sentinel 系统所监控的主服务器。这个时候大家可能就会有疑问了，Sentinel 不是还监控主服务下面的所有从服务器吗，为什么 sentinelState 这个数据结构没有 slaves 属性？这是因为 slaves 被保存到 sentinelRedisInstance 结构了。</p><h3 id="sentinelRedisInstance"><a href="#sentinelRedisInstance" class="headerlink" title="sentinelRedisInstance"></a>sentinelRedisInstance</h3><p>上面提到 Sentinel 监控的 masters 信息被保存在 sentinelRedisInstance 这个数据结构中。其实不仅是 master , slave 和 Sentinel 系统中的其他 Sentinel 节点信息也是用这个数据结构来保存，只是不同的角色所用到的属性不太一样。现在我们来看下这个数据结构的几个关键属性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">struct sentinelRedisInstance &#123;</span><br><span class="line">// 标识值，记录了实例的类型，以及该实例的当前状态</span><br><span class="line">int flags;</span><br><span class="line"></span><br><span class="line">// 实例的名字</span><br><span class="line">// 主服务器的名字由用户在配置文件中指定</span><br><span class="line">// 从服务器以及 Sentinel 的名字由 Sentinel 自动设置，格式为 IP:PORT</span><br><span class="line">char *name;</span><br><span class="line"></span><br><span class="line">// 实例运行ID</span><br><span class="line">char *runid;</span><br><span class="line"></span><br><span class="line">// 配置纪元，用于实现故障转移</span><br><span class="line">uint64_t config_epoch;</span><br><span class="line"></span><br><span class="line">// 实例的地址，这个数据结构保存的是实例的 IP 和 PORT</span><br><span class="line">sentinelAddr *addr;</span><br><span class="line"></span><br><span class="line">// 实例多少毫秒没响应被认为主观下线，由 SENTINEL down-after-milliseconds 指定</span><br><span class="line">mstime_t down_after_period;</span><br><span class="line"></span><br><span class="line">// 多少个实例认为主观下线之后变为客观下线，即 SENTINEL monitor &lt;master-name&gt; &lt;ip&gt; &lt;port&gt; &lt;quorum&gt; 中的 quorum 参数</span><br><span class="line">int quorum;</span><br><span class="line"></span><br><span class="line">// 在执行故障转移时，可以同时对新的主服务器进行同步的从服务器数量</span><br><span class="line">int parallel_syncs;</span><br><span class="line"></span><br><span class="line">// 故障转移超时时间</span><br><span class="line">mstime_t failover_timeout;</span><br><span class="line"></span><br><span class="line">// 这个属性只有 master 才有，保存的是该 master 下属的所有 slave。key 是 ip:port，value 是 sentinelRedisInstance 指针</span><br><span class="line">dict *slaves;</span><br><span class="line"></span><br><span class="line">// 这个属性只有 master 才有，保存的是监控该 master的所有 Sentinel。key 是 ip:port，value 是 sentinelRedisInstance 指针</span><br><span class="line">dict *sentinels;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>核心的两个数据结构我们已经有所了解，为了让大家更有体感，我们来画一下具有三个 Sentinel 和一主两备的 Redis 实例的结构图。</p><p>假设我们 sentinel.conf 配置文件中配置的 master 名字为 mymaster。</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/sentinel-deploy.png" alt="Sentinel deployment"></p><p>上面是部署结构图，三个 Sentinel 和一主两备的 Redis 实例，下面是数据结构。</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/sentinel-datastruct.png" alt="Sentinel data struct"></p><h3 id="获取主服务器信息"><a href="#获取主服务器信息" class="headerlink" title="获取主服务器信息"></a>获取主服务器信息</h3><p>看完上面的示例，大家可能会有个疑问，在初始化的时候，我们配置文件只配置了 master 的相关信息，并没有配置 slave 的信息，Sentinel 是怎么获取到 slave 的信息并保存在数据结构中的呢？</p><p>回想一下主从复制的部署过程，从服务会通过 SLAVEOF 命令来复制主服务器，建立了双方之间的联系，此时主服务器是有 slave 服务器的相关信息。所以，在 Sentinel 初始化的时候并不需要那么麻烦把 slave 服务器也配置到 sentinel.conf 配置文件中。</p><p>Sentinel 初始化的最后一步是创建到主服务器的网络连接，包含命令连接和订阅连接：</p><ul><li>命令连接：用于向主服务发送命令，并接收命令回复</li><li>订阅连接：用于订阅主服务器的 <code>__sentinel__:hello</code> 频道，这个作用后面会介绍</li></ul><p>Slave 服务器的信息就是通过命令连接来发现的，Sentinel 默认会以十秒一次的频率，通过向主服务发送 INFO 命令，主要可以获得以下两方面的信息：</p><ul><li>主服务器本身的信息，如 run_id 和 role</li><li>所有从服务器信息，包含 ip 和 port</li></ul><p>根据这些返回的信息，就可以更新上面的数据结构中的内容了。</p><h3 id="获取从服务器信息"><a href="#获取从服务器信息" class="headerlink" title="获取从服务器信息"></a>获取从服务器信息</h3><p>从主服务器信息中获取到所有从服务器的 ip 和 port 之后，Sentinel 也会创建到从所有从服务器的命令连接和订阅连接，也就是说 Sentinel 对所有监控的 Redis 服务器（不管主从）都会建立命令连接和订阅连接。</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/sentinel-link.png" alt="Sentinel network"></p><p>创建命令连接之后，也是十秒一次的频率向从服务器发送 INFO 命令获取相关信息：</p><ul><li>从服务器 run_id, role, slave_priority, slave_repl_offset</li><li>主服务器 master_host, master_port</li><li>主从服务器连接状态 master_link_status</li></ul><p>获取到这些信息之后更新从服务器的 sentinelRedisInstance。</p><h3 id="向主服务器和从服务器发送消息"><a href="#向主服务器和从服务器发送消息" class="headerlink" title="向主服务器和从服务器发送消息"></a>向主服务器和从服务器发送消息</h3><p>现在，我们已经知道 Sentinel 怎么发现 slave 服务器的了，但是现在另外一个疑问来了，Sentinel 是怎么发现其他 Sentinel 的呢？Sentinel 之间是需要互相通信的，这样才能进行故障转移等工作，所以 Sentinel 之间必须能够互相发现。</p><p>答案就是通过上面所说的订阅链接，从订阅的 <code>__sentinel__:hello</code> 频道接收到信息，这些信息包含了其他 Sentinel 的信息。那么 <code>__sentinel__:hello</code> 频道里面的信息是谁发送的呢？必然需要有客户端向 <code>__sentinel__:hello</code> 频道发送了消息，Sentinel 才能收到消息。</p><p>既然是通过订阅连接发现其他 Sentinel ，只有 Sentinel 本身知道自己的信息，所以转了一圈发送信息到 <code>__sentinel__:hello</code> 频道的也是 Sentinel。简单来说就是 Sentinel 通过 PUBLISH 命令向所有被监控的主服务器和从服务器的 <code>__sentinel__:hello</code> 频道发送消息，告诉别的 Sentinel 自身的相关信息。</p><p>在默认情况下，Sentinel 会以两秒一次的频率，通过命令连接向所有被监控的主服务器和从服务器发送以下格式的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PUBLISH `__sentinel__:hello` &quot;&lt;s_ip&gt;,&lt;s_port&gt;,&lt;s_runid&gt;,&lt;s_epoch&gt;,&lt;m_name&gt;,&lt;m_ip&gt;,&lt;m_port&gt;,&lt;m_epoch&gt;&quot;</span><br></pre></td></tr></table></figure><p>其中 s_ 开头的参数是 Sentinel 本身的信息，而 m_ 开头的参数是主服务器的信息。</p><p>以我们上面的例子来说，当 Sentinel1， Sentinel2， Sentinel3， 都向他们所监控的所有主服务器和从服务器发送了 PUBLISH 命令之后，三个 Sentinel 都会通过订阅连接获取到各自发送的消息，包含自己发送的那一条。当一个 Sentinel 从 <code>__sentinel__:hello</code> 频道收到一条消息时，会对这条消息进行分析：</p><ul><li>如果 Sentinel runid 跟自己的 runid 相同，证明是自己发送的，则不作处理</li><li>如果不同，证明是其他 Sentinel 发送过来的，然后就更新数据结构</li></ul><h3 id="创建连向其他-Sentinel-的命令连接"><a href="#创建连向其他-Sentinel-的命令连接" class="headerlink" title="创建连向其他 Sentinel 的命令连接"></a>创建连向其他 Sentinel 的命令连接</h3><p>通过 <code>__sentinel__:hello</code> 频道发现了其他的 Sentinel ，那么为了和其他 Sentinel 通信，需要建立与其他 Sentinel 的命令连接。最终监控同一个主服务的多个 Sentinel 将形成互相连接的网络：</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/sentinel-network.png" alt="Sentinel network"></p><p>通过命令连接相连的各个 Sentinel 可以通过向其他 Sentinel 发送命令请求来进行信息交换，下一篇文章要介绍的检测主观下线、客观下线就是通过信息交换来实现的。</p><p>这里要注意的是，Sentinel 在连接主服务器和从服务器时，会同时创建命令连接和订阅连接，但是在连接其他 Sentinel 时，却只会创建命令连接，因为他们已经互相发现了，没必要用订阅连接。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天的文章由于篇幅关系其实还没有讲到核心功能，检测服务器状态和故障转移，但是有了这些基础之后就很简单了，现在做一个小结：</p><ul><li>Sentinel 是一个特殊的 Redis 服务器，通过配置文件获取要监控的 master 信息</li><li>Sentinel 通过向主服务发送 INFO 命令获取到 slave 信息</li><li>Sentinel 通过订阅 <code>__sentinel__:hello</code> 频道获取其他 Sentinel 信息</li><li>Sentinel 会向主服务和所属的从服务器建立命令连接和订阅连接，每十秒发送 INFO 命令，每两秒发送 PUBLISH 命令</li><li>Sentinel 之间只会建立命令连接</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上一篇文章研究了一下 Redis 的主从复制，如果再进一步思考，假设发生了主从切换的情况，客户端是怎么感知到，从而连接到主服务器进行读写操作呢？&lt;/p&gt;
&lt;p&gt;今天我们要研究的 Sentinel(哨兵) 模式就是 Redis 高可用的一种方案：由一个或多个 Sentinel
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>redis 主从复制</title>
    <link href="http://ideajava.com/2019/12/24/redis-replication/"/>
    <id>http://ideajava.com/2019/12/24/redis-replication/</id>
    <published>2019-12-24T07:55:51.000Z</published>
    <updated>2020-01-07T12:25:49.948Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间看了 《数据密集型应用系统设计》数据复制相关章节，相对来说比较理论，现在来看一个实际的例子，Redis 的主从复制。</p><p>在 Redis 中，可以通过 SLAVEOF 命令来让一个服务器去复制另一个服务器，被复制的服务器称为 Master，复制的服务器称为 Slave。</p><h3 id="复制的实现"><a href="#复制的实现" class="headerlink" title="复制的实现"></a>复制的实现</h3><p>Redis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作：</p><ul><li>同步操作用于将从服务器的数据库状态更新至主服务器当前所处的数据库状态。</li><li>命令传播操作用于在主服务器的数据库状态被修改，导致主从服务器的数据库状态出现不一致时，让主从服务器的数据库重新回到一致性状态。</li></ul><h4 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h4><p>当客户端向从服务器发送 SLAVEOF 命令，要求从服务器复制主服务器时，从服务器首先要执行同步操作，将从服务器的数据库状态更新至主服务器当前所处于的数据库状态。</p><p>同步操作分为全量同步和部分同步，在 Redis 2.8 之前只有全量同步（SYNC），Redis 2.8 之后有了部分同步 (PSYNC)，Redis 4.0 之后对部分同步进行了优化 (为了区别称为 PSYNC2)。</p><p>我们先来看一下全量同步的过程：</p><ol><li>从服务器向主服务器发送 SYNC 命令。</li><li>收到 SYNC 命令的主服务器执行 BGSAVE 命令，在后台生成一个 RDB 文件，并使用一个缓冲区记录从现在开始执行的所有写命令。</li><li>当主服务器的 BGSAVE 命令执行完毕时，主服务器会将 BGSAVE 命令生成的 RDB 文件发送给从服务器，从服务器接收并载入这个 RDB 文件，将自己的数据库状态更新至主服务器执行 BGSAVE 命令时的数据库状态。</li><li>主服务器将记录在缓冲区里面的所有写命令发送给从服务器，从服务器执行这些写命令。</li></ol><h4 id="命令传播"><a href="#命令传播" class="headerlink" title="命令传播"></a>命令传播</h4><p>在同步操作执行完毕之后，主从服务器两者的数据库将达到一致状态，但这种一致并不是一成不变的，每当主服务器执行客户端发送的写命令时，主服务器的数据库就有可能会被修改，并导致主从服务器不再一致。</p><p>为了让主从服务器再次回到一致状态，主服务器需要对从服务器执行命令传播操作：主服务器会将自己执行的写命令发送给从服务器执行，当从服务器执行了相同的写命令之后，主从服务器将再次回到一致状态。</p><h4 id="部分同步"><a href="#部分同步" class="headerlink" title="部分同步"></a>部分同步</h4><p>相信大家都能想到，如果从服务器因为网络中断了比较短的时间，此时主从服务器的数据库差别可能并不是特别大，每次都要全量同步的话就非常浪费资源了。</p><p>在 Redis 2.8 之后引入了部分同步（PSYNC），部分同步功能由以下三个部分构成：</p><ol><li>主服务器的复制偏移量（replication offset）和从服务器的复制偏移量。</li><li>主服务器的复制积压缓冲区（replication backlog）。</li><li>服务器的运行 ID（run ID）。</li></ol><h5 id="复制偏移量"><a href="#复制偏移量" class="headerlink" title="复制偏移量"></a>复制偏移量</h5><p>执行复制的双方————主服务器和从服务器会分别维护一个复制偏移量：</p><ul><li>主服务器每次向从服务器传播 N 个字节的数据时，就将自己的复制偏移量的值加上 N。</li><li>从服务器每次收到主服务器传播来的 N 个字节数据时，就将自己的复制偏移量的值加上 N。</li></ul><h5 id="复制积压缓冲区"><a href="#复制积压缓冲区" class="headerlink" title="复制积压缓冲区"></a>复制积压缓冲区</h5><p>复制积压缓冲区是由主服务器维护的一个固定长度先进先出队列，默认大小为 1MB。</p><p>当主服务器进行命令传播时，它不仅会将写命令发送给所有从服务器，还会将写命令入队到复制积压缓冲区里面。因此，主服务器的复制积压缓冲区里面会保存一部分最近传播的写命令，并且复制积压缓冲区会为队列中的每个字节记录相应的复制偏移量。</p><p>当从服务器重新连上主服务器时，从服务器会通过 PSYNC 命令将自己的复制偏移量 offset 发送给主服务器，主服务器会根据这个复制偏移量是否在复制积压缓冲区内来决定执行部分同步还是全量同步。</p><p><strong>注意：</strong> 复制积压缓冲区的大小需要根据实际情况调整，可以根据 second <em> write_size_per_second 来估算，为了安全起见可以设置为 2 </em> second * write_size_per_second。避免复制积压缓冲区溢出导致全量部分。</p><h5 id="运行-ID"><a href="#运行-ID" class="headerlink" title="运行 ID"></a>运行 ID</h5><p>除了复制偏移量和复制积压缓冲区之外，实现部分同步还需要用到服务器运行 ID（run ID）。</p><ul><li>每个 Redis 服务器，不论主服务器还是从服务器，都会有自己的运行 ID。</li><li>运行 ID 在服务器启动时自动生成，由 40 个随机的十六进制字符组成，例如 3426cd01f39d5ee2dc48969641423b0758b9d8fd。</li></ul><p>当从服务器对主服务器进行初次复制时，主服务器会将自己的运行 ID 传送给从服务器，而从服务器则会将这个运行 ID 保存起来。</p><p>当从服务器断线并重新连上一个主服务器时，从服务器将向当前连接的主服务器发送之前保存的运行 ID：如果从服务器保存的运行 ID 和当前连接的主服务器的运行 ID 相同，那么说明从服务器断线之前复制的就是当前连接的这个主服务器，那么就可以尝试执行部分同步操作。</p><h5 id="PSYNC-命令的实现"><a href="#PSYNC-命令的实现" class="headerlink" title="PSYNC 命令的实现"></a>PSYNC 命令的实现</h5><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/psync.png" alt="Data replicate"></p><h4 id="部分同步优化"><a href="#部分同步优化" class="headerlink" title="部分同步优化"></a>部分同步优化</h4><p>大家应该也很容易想到上面的部分同步功能存在的缺陷，如果存在主从切换的情况，部分同步就无法实现了，因为主服务器变了，runid 不一样了只能执行全量同步了。</p><p>为了应对主从切换这种情况，Redis 4.0 对 PSYNC 进行了优化，引入了 master_replid 和 master_replid2 两个 ID。</p><ul><li>master_replid 指的是当前的复制 ID。与当前主服务器的 master_replid 一致。</li><li>master_replid2 指的是上一次的复制 ID。与上一次同步的主服务器的 master_replid 一致。</li></ul><p>master_replid 和 master_replid2 跟 runid 没有关系，只是生成的规则一样。也就是之前是根据 runid 是否相等来判断，优化之后根据 master_replid 或者 master_replid2 是否相等来判断。</p><p>试想一下，当主服务器挂了，从服务被提升为主服务器，此时的新主服务器 master_replid 是新生成的，master_replid2 就是上一次同步的主服务器的 master_replid ，也就挂掉哪台的 master_replid。当挂掉的服务器起来之后，角色变成了从服务器，此时用之前的 master_replid 来请求数据同步，新的主服务器会判断是否与 master_replid 或者 master_replid2 相等以及 offset 来判断是否可以进行部分同步。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Redis 2.8 之前只有全量同步</li><li>Redis 2.8 之后通过复制偏移量，复制积压缓冲区，服务器运行 ID 引入了部分同步</li><li>Redis 4.0 之后通过 master_replid 和 master_replid2 解决主备切换全量同步的问题</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://www.jianshu.com/p/54dabc470eb6" target="_blank" rel="noopener">Redis 4.0 解决全量同步问题</a></li><li><a href="https://www.cnblogs.com/kismetv/p/9236731.html" target="_blank" rel="noopener">Redis 主从复制</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前段时间看了 《数据密集型应用系统设计》数据复制相关章节，相对来说比较理论，现在来看一个实际的例子，Redis 的主从复制。&lt;/p&gt;
&lt;p&gt;在 Redis 中，可以通过 SLAVEOF 命令来让一个服务器去复制另一个服务器，被复制的服务器称为 Master，复制的服务器称为
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>kube-proxy 与 service 之间的那些事</title>
    <link href="http://ideajava.com/2019/11/29/kube-proxy-and-service/"/>
    <id>http://ideajava.com/2019/11/29/kube-proxy-and-service/</id>
    <published>2019-11-29T09:58:54.000Z</published>
    <updated>2019-12-02T06:11:05.757Z</updated>
    
    <content type="html"><![CDATA[<p>在 k8s 中，service 是一个和重要的概念。service 为一组相同的 pod 提供了统一的入口，并且能达到负载均衡的效果。</p><p>service 具有这样的功能，正是 kube-proxy 的功劳。当我们暴露一个 service 的时候，kube-proxy 会在 iptables 中追加一些规则，为我们实现了路由与负载均衡的功能。</p><p>下面用一个具体的小例子来看看 service 是怎么路由的。</p><h3 id="KUBE-SERVICES-链"><a href="#KUBE-SERVICES-链" class="headerlink" title="KUBE-SERVICES 链"></a>KUBE-SERVICES 链</h3><p>在我搭建的一个 minikube 环境中，运行 <code>iptables -t nat -nvL</code> 命令查看 iptables 中的内容。</p><p>首先，看下 PREROUTING 的规则</p><p><code>Chain PREROUTING (policy ACCEPT 5 packets, 271 bytes) pkts bytes target     prot opt in     out     source               destination 130K 6355K KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */ 121K 5633K DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL</code></p><p>可以看出，所有请求都会先通过 k8s 追加的 KUBE-SERVICES 链，我们再看一下 KUBE-SERVICES 链中有什么规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443</span><br><span class="line">    4   422 KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-JD5MR3NA4I4DYORP  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span><br><span class="line">    0     0 KUBE-SVC-MW5CXKKSAWLC5IQG  tcp  --  *      *       0.0.0.0/0            10.102.64.24         /* default/device: cluster IP */ tcp dpt:8080</span><br><span class="line">    0     0 KUBE-SVC-LKM3CLHDXK6GWQVY  tcp  --  *      *       0.0.0.0/0            10.100.159.228       /* default/traffic: cluster IP */ tcp dpt:8080</span><br><span class="line">    0     0 KUBE-SVC-2Q6H6PHDBHDVLI7U  tcp  --  *      *       0.0.0.0/0            10.99.126.231        /* default/gateway: cluster IP */ tcp dpt:8080</span><br><span class="line">   53  2720 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure><p>可以看到，有一系列目标为 KUBE-SVC-xxx 链的规则，每条规则都会匹配某个目标 ip 与端口。也就是说访问某个 ip:port 的请求会由 KUBE-SVC-xxx 链来处理。</p><p>这个目标 IP ，其实就是我 minikube 里面发布的 service ip，用 kubectl get svc -o wide 查看一下 demo 中的服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE     SELECTOR</span><br><span class="line">device    NodePort    10.102.64.24     &lt;none&gt;        8080:31748/TCP   5d19h   app=device</span><br><span class="line">gateway   ClusterIP   10.99.126.231    &lt;none&gt;        8080/TCP         2d1h    app=gateway</span><br><span class="line">traffic   ClusterIP   10.100.159.228   &lt;none&gt;        8080/TCP         2d19h   app=traffic</span><br><span class="line">kubernetes       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP          7d7h    &lt;none&gt;</span><br></pre></td></tr></table></figure><p>比如 gateway 服务的 CLUSTER-IP 是 10.99.126.231，端口 8080。那么按照上面的 KUBE-SERVICES 链的规则，target 是 KUBE-SVC-2Q6H6PHDBHDVLI7U。我们再看一下 KUBE-SVC-2Q6H6PHDBHDVLI7U 链的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-SVC-2Q6H6PHDBHDVLI7U (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SEP-2S4ZDBRDPHRMTYD5  all  --  *      *       0.0.0.0/0            0.0.0.0/0</span><br></pre></td></tr></table></figure><p>继续看下 KUBE-SEP-2S4ZDBRDPHRMTYD5 链的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-SEP-2S4ZDBRDPHRMTYD5 (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  all  --  *      *       172.17.0.8           0.0.0.0/0</span><br><span class="line">    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:172.17.0.8:8080</span><br></pre></td></tr></table></figure><p>会发现这条链做了一次目标网络地址转换 DNAT，将原来的的目标网络地址 service 的 CLUSTER-IP 10.99.126.231 端口 8080 转换成了 172.17.0.8:8080。此时，我们用 <code>kubectl get pod -o wide</code> 命令看一下 demo 中 pod 的 IP 地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NAME                               READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">device-5b6d58dc76-ddtsm     1/1     Running   0          8h      172.17.0.10   minikube   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">device-5b6d58dc76-dk4kq     1/1     Running   0          2d23h   172.17.0.5    minikube   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">gateway-594c8d6cb5-25mc6    1/1     Running   7          2d5h    172.17.0.8    minikube   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">traffic-68cbcb88b9-qqrtk    1/1     Running   0          3d      172.17.0.9    minikube   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>172.17.0.8 就是我们 gateway service 的 endpoint，也就是我们想要调用的 pod。这就是为什么我们调用 service 的 CLUSTER-IP 和端口能访问到 pod 的原因了。</p><h3 id="KUBE-SEP-xxx-链"><a href="#KUBE-SEP-xxx-链" class="headerlink" title="KUBE-SEP-xxx 链"></a>KUBE-SEP-xxx 链</h3><p>上面的 KUBE-SEP-xxx 链其实就是 service 的 endpoint，那么 service 怎么实现负载均衡的呢。道理其实很简单，一个 service 的链对应多个 endpoint 即可，然后按照一定的比例随机调动。我在 demo 中部署了两个 device 服务的 endpoint，我们我看 kube-proxy 为我们生成的规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-SVC-MW5CXKKSAWLC5IQG (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SEP-B7QXYLUMAA6RGM2S  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000</span><br><span class="line">    0     0 KUBE-SEP-EZG7Q3GTCBMWXIRX  all  --  *      *       0.0.0.0/0            0.0.0.0/0</span><br></pre></td></tr></table></figure><p>50% 的概率会到 KUBE-SEP-B7QXYLUMAA6RGM2S，50% 概率到 KUBE-SEP-EZG7Q3GTCBMWXIRX。这两个目标正是 device 的两个 pod，如下所示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-SEP-B7QXYLUMAA6RGM2S (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  all  --  *      *       172.17.0.10          0.0.0.0/0</span><br><span class="line">    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:172.17.0.10:8080</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-SEP-EZG7Q3GTCBMWXIRX (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  all  --  *      *       172.17.0.5           0.0.0.0/0</span><br><span class="line">    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:172.17.0.5:8080</span><br></pre></td></tr></table></figure><h3 id="KUBE-NODEPORTS-链"><a href="#KUBE-NODEPORTS-链" class="headerlink" title="KUBE-NODEPORTS 链"></a>KUBE-NODEPORTS 链</h3><p>回头再看看我们最初入口处的 KUBE-SERVICES，首先会判断是否匹配到了某个服务 KUBE-SVC-xxx。如果没有匹配到，最后还有 KUBE-NODEPORTS 链，匹配目标地址类型属于主机系统的本地网络地址的数据包。我们看下 demo 中 KUBE-NODEPORTS 链的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain KUBE-NODEPORTS (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/device: */ tcp dpt:31748</span><br><span class="line">    0     0 KUBE-SVC-MW5CXKKSAWLC5IQG  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/device: */ tcp dpt:31748</span><br></pre></td></tr></table></figure><p>会发现只要是目标端口为 31748 的请求都会交给  KUBE-SVC-MW5CXKKSAWLC5IQG 链来处理，这条链就是我们 device service 。其实 31748 这个端口，就是我将 device 服务以 NodePort 类型发布随机分配的端口。看到这里，大家应该明白了为什么通过 NODEPORT 暴露的服务能在集群外部访问了。</p><p>集群外部：通过节点 IP 加 nodePort 可以访问到 device service<br>集群内部：通过 device service 的 CLUSTER-IP 和 port 能访问到 device service</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>kube-proxy 会为我们暴露的 service 追加 iptables 规则</li><li>所有进出请求都会经过 KUBE-SERVICES 链</li><li>KUBE-SERVICES 链有我们发布的每一个服务，每个服务的链会对应到 DNAT 到 service 的 endpoint</li><li>KUBE-SERVICES 链最后的是 KUBE-NODEPORTS 链，会匹配请求本主机的一些端口，这就是我们通过 NODEPORT 类型发布服务能在集群外部访问的原因</li></ol><h3 id="延伸资料"><a href="#延伸资料" class="headerlink" title="延伸资料"></a>延伸资料</h3><p><a href="https://www.zsythink.net/archives/1199" target="_blank" rel="noopener">iptables 入门</a><br><a href="https://www.cnblogs.com/zclzhao/p/5081590.html" target="_blank" rel="noopener">iptables 命令，规则介绍</a><br><a href="https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/03/27/Kubernetes-kube-proxy.html" target="_blank" rel="noopener">kube-proxy 转发规则</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在 k8s 中，service 是一个和重要的概念。service 为一组相同的 pod 提供了统一的入口，并且能达到负载均衡的效果。&lt;/p&gt;
&lt;p&gt;service 具有这样的功能，正是 kube-proxy 的功劳。当我们暴露一个 service 的时候，kube-pro
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>将应用从 SpringCloud 迁移到 k8s</title>
    <link href="http://ideajava.com/2019/11/27/SpringCloud-to-K8S/"/>
    <id>http://ideajava.com/2019/11/27/SpringCloud-to-K8S/</id>
    <published>2019-11-27T02:41:56.000Z</published>
    <updated>2019-11-27T13:58:14.623Z</updated>
    
    <content type="html"><![CDATA[<p>最近花了几天时间看了一下 k8s 和 istio 的文档，对容器化运维以及服务网格有了基础的了解。俗话说读万卷书不如行万里路，于是先尝试用 minikube 练一下手，将现有了一个 Spring Cloud 项目迁移到 k8s 上来。</p><p>粗略地整理了一个整个流程，主要有以下几个改动点：</p><ol><li>代码与配置修改</li><li>编写 Dockerfile</li><li>安装 kubectl 和 minikube</li><li>创建 configmap 资源</li><li>创建 deployment 资源</li><li>创建 secret 资源</li><li>暴露服务给集群外部调用</li></ol><h3 id="代码与配置修改"><a href="#代码与配置修改" class="headerlink" title="代码与配置修改"></a>代码与配置修改</h3><p>代码与配置的修改点不多，这样充分说明从 SpringCloud 迁移到 k8s 是相当的友好。</p><ol><li>由于 k8s 有内置 dns 做服务地址解析以及 service 资源做负载均衡。所以我们不再需要 eureka 做服务注册与发现了，需要把 eureka 相关的依赖与注解删除掉。</li></ol><p>删除 eureka client 的 @EnableDiscoveryClient 注解以及以下依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>另外 <code>application.yml</code> 配置文件也要去掉 eureka server 相关配置。</p><ol start="2"><li>由于 k8s 服务之间的调用是通过服务名称:端口的形式，所以 feign client 上需用加上 url 参数，例如：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@FeignClient(value = &quot;xxxService&quot;, url = &quot;xxxService:8080&quot;)</span><br></pre></td></tr></table></figure><p>xxxService 就是后面要部署的 k8s 服务名称，8080 端口是 xxxService k8s 服务暴露的端口，可以自定义。</p><ol start="3"><li>为了我们修改 springboot 的 application.yml 文件之后，k8s 能自动感知然后重新自动部署应用，我们可以用 configmap 资源来维护我们的配置文件。需要作出以下改动：</li></ol><p>微服务内部只保留 bootstrap.yml 文件，其他 application.yml 文件在统一的地方维护后续用于生成 configmap 资源。bootstrap.yml 文件示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  application:</span><br><span class="line">     name: xxxService</span><br><span class="line">  profiles:</span><br><span class="line">    active: #spring.profiles.active#</span><br><span class="line">  servlet:</span><br><span class="line">    multipart:</span><br><span class="line">      max-request-size: 100MB</span><br><span class="line">      max-file-size: 100MB</span><br><span class="line">  devtools:</span><br><span class="line">    restart:</span><br><span class="line">      enabled: true   # 设置开启热部署</span><br><span class="line">  cloud:</span><br><span class="line">    kubernetes:</span><br><span class="line">      reload:</span><br><span class="line">        enabled: true</span><br><span class="line">        mode: polling</span><br><span class="line">        period: 5000</span><br><span class="line">      config:</span><br><span class="line">        sources:</span><br><span class="line">          - name: $&#123;spring.application.name&#125;</span><br><span class="line">management:</span><br><span class="line">  endpoint:</span><br><span class="line">    restart:</span><br><span class="line">      enabled: true</span><br><span class="line">    health:</span><br><span class="line">      enabled: true</span><br><span class="line">    info:</span><br><span class="line">      enabled: true</span><br></pre></td></tr></table></figure><p>新增以下依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-cloud-starter-kubernetes-config&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0.1.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>这个依赖的用途就是让 k8s 来管理我们的配置文件。</p><h3 id="编写-Dockerfile"><a href="#编写-Dockerfile" class="headerlink" title="编写 Dockerfile"></a>编写 Dockerfile</h3><p>我们需要将 springboot 应用打包成镜像，然后上传到镜像仓库中，后面部署到 k8s 中需要指定镜像地址。Dockerfile 文件示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM openjdk:8-jdk-alpine</span><br><span class="line">VOLUME /tmp</span><br><span class="line">ADD xxxService-1.0.jar app.jar</span><br><span class="line">ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]</span><br></pre></td></tr></table></figure><p>将镜像打包到仓库这里就不展开了。</p><h3 id="安装-kubectl-和-minikube"><a href="#安装-kubectl-和-minikube" class="headerlink" title="安装 kubectl 和 minikube"></a>安装 kubectl 和 minikube</h3><p>这个步骤可以参考<a href="https://kubernetes.io/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">官方文档</a></p><h3 id="创建-configmap-资源"><a href="#创建-configmap-资源" class="headerlink" title="创建 configmap 资源"></a>创建 configmap 资源</h3><p>假设我们将 springboot 的配置文件维护在 git 仓库中，我们就可以在安装 minikube 的机器上 git clone 下来，运行以下命令，就可以将配置文件维护在 configmap 中了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap xxx --from-file=application.yml</span><br></pre></td></tr></table></figure><p>但是后面我们部署服务的时候，默认账户下 pod 应该是没有权限读取到 configmap 中的内容的，所以我们需要创建有权限的 serviceaccount。可以参考一下<a href="https://juejin.im/post/5d71b1b4518825462823825a" target="_blank" rel="noopener">这篇文章</a></p><h3 id="创建-deployment-资源"><a href="#创建-deployment-资源" class="headerlink" title="创建 deployment 资源"></a>创建 deployment 资源</h3><p>创建 deployment 资源很简单，只要指定镜像地址，运行一个简单的命令即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment xxxService --image=镜像地址</span><br></pre></td></tr></table></figure><p>但是这样默认创建的 deployment 资源还是有点小问题的：</p><ol><li>还没有配置读取我们上面创建的 configmap</li><li>由于我用的是阿里云私有镜像仓库，没有配置用户密码会拉取镜像失败</li></ol><p>对于第一个问题，我们需要将 configmap 挂在到容器中。参考配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: demo</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: demo</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: demo</span><br><span class="line">        image: xxx</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /deployments/config</span><br><span class="line">          name: easygo-device</span><br><span class="line">          readOnly: true</span><br><span class="line">      volumes:</span><br><span class="line">      - name: demo-config</span><br><span class="line">        configMap:</span><br><span class="line">          name: demo-configmap</span><br><span class="line">          items:</span><br><span class="line">            - key: application.yml</span><br><span class="line">              path: application.yml </span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: aliyun-docker-hub</span><br></pre></td></tr></table></figure><p>对于第二问题，我们需要创建 secret 资源来保存我们私有仓库的用户名密码，然后配置在 deployment 资源上，即上面的 imagePullSecrets 属性。创建 secret 资源比较简单，见下面的命令。</p><h3 id="创建-secret-资源"><a href="#创建-secret-资源" class="headerlink" title="创建 secret 资源"></a>创建 secret 资源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret docker-registry 资源名称 --docker-server=仓库地址 --docker-username=&#123;UserName&#125; --docker-password=&#123;Password&#125; --docker-email=&#123;Email&#125;</span><br></pre></td></tr></table></figure><p>资源名称就是上面配置的 aliyun-docker-hub ，名称可自定义。</p><h3 id="暴露服务给集群外部调用"><a href="#暴露服务给集群外部调用" class="headerlink" title="暴露服务给集群外部调用"></a>暴露服务给集群外部调用</h3><p>默认情况下，k8s 集群外部是无法访问到集群内部的服务，所以我们需要将服务暴露出去，方式有很多种。具体可以看下<a href="https://jimmysong.io/posts/accessing-kubernetes-pods-from-outside-of-the-cluster/" target="_blank" rel="noopener">这篇文章</a></p><p>最简单的方式是 NodePort，比如将 xxx 服务暴露出去，运行以下命令即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment xxx --type=NodePort --port=8080</span><br></pre></td></tr></table></figure><p>然后我们就可以通过节点IP加上面暴露的端口即可访问。</p><p>而常用的方式是通过 Ingress 资源暴露，需要创建 Ingress 资源和部署 Ingress Controller。</p><p>先看一下官方的 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">Ingrss 介绍</a></p><p>然后 Ingress Controller 可以用 nginx ingress，部署方式见<a href="https://kubernetes.github.io/ingress-nginx/deploy/" target="_blank" rel="noopener">官方文档</a></p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>本文只是介绍了 SpringCloud 迁移到 k8s 的大致流程，不是很完善，按照上面的步骤来弄应该还是会踩点坑。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近花了几天时间看了一下 k8s 和 istio 的文档，对容器化运维以及服务网格有了基础的了解。俗话说读万卷书不如行万里路，于是先尝试用 minikube 练一下手，将现有了一个 Spring Cloud 项目迁移到 k8s 上来。&lt;/p&gt;
&lt;p&gt;粗略地整理了一个整个流程
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>数据复制——多主</title>
    <link href="http://ideajava.com/2019/10/30/mutiple-master-node-data-copy/"/>
    <id>http://ideajava.com/2019/10/30/mutiple-master-node-data-copy/</id>
    <published>2019-10-30T07:31:55.000Z</published>
    <updated>2019-10-30T07:50:46.290Z</updated>
    
    <content type="html"><![CDATA[<p>分布式造飞机理论大全——《数据密集型应用系统设计》，读书笔记 2.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/mutiple-master-node-data-copy.png" alt="Data replicate"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分布式造飞机理论大全——《数据密集型应用系统设计》，读书笔记 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/rason/rason.github.io/master/image/mutiple-master-no
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>数据复制——主从</title>
    <link href="http://ideajava.com/2019/10/21/Data-replicate-master-slave/"/>
    <id>http://ideajava.com/2019/10/21/Data-replicate-master-slave/</id>
    <published>2019-10-21T12:12:42.000Z</published>
    <updated>2019-10-21T12:20:41.802Z</updated>
    
    <content type="html"><![CDATA[<p>最近在研读分布式造飞机理论大全——《数据密集型应用系统设计》，做了一点读书笔记（抄书），偷懒直接上图了。</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/data-replicate-master-slave.png" alt="Master slave"></p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/data-replicate.png" alt="Data replicate"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在研读分布式造飞机理论大全——《数据密集型应用系统设计》，做了一点读书笔记（抄书），偷懒直接上图了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/rason/rason.github.io/master/imag
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>数据库隔离级别温故而知新</title>
    <link href="http://ideajava.com/2019/09/26/transaction-isolation/"/>
    <id>http://ideajava.com/2019/09/26/transaction-isolation/</id>
    <published>2019-09-26T12:48:55.000Z</published>
    <updated>2019-09-27T08:30:57.110Z</updated>
    
    <content type="html"><![CDATA[<p>在工作中，碰到一些问题的时候你会想，诶，这个知识点我似乎没理解透。数据库的隔离级别就是我最近有点困惑的知识点，大家试着回答以下问题：</p><ol><li><p>数据库的隔离级别究竟是为什么解决什么出题而出现的？</p></li><li><p>数据库的隔离级别是怎么实现的？</p></li><li><p>数据库的隔离级别跟锁有什么关系，有了数据库的隔离级别，我们平时在写 SQL 的时候还需要手工加锁吗？</p></li><li><p>为什么 MySQL 的 MVCC 机制在 RC 隔离级别下没有解决不可重复读的问题？</p></li></ol><p>…</p><p>不断地去思考，不断地去刨根问底，我们才能真正的把一个知识理解透。 </p><h4 id="数据库的隔离级别"><a href="#数据库的隔离级别" class="headerlink" title="数据库的隔离级别"></a>数据库的隔离级别</h4><p>我们先来看一下数据库定义的几种隔离级别，偷个懒直接网上找个图：</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/isolation-level.jpg" alt="isolation-level"></p><p>我们发现，隔离级别的关键字是<strong>“读”</strong>。也就是说，隔离级别的作用是为了有效保证并发<strong>读取</strong>数据的正确性。这就回答了第一个问题。</p><h4 id="数据库的隔离级别实现"><a href="#数据库的隔离级别实现" class="headerlink" title="数据库的隔离级别实现"></a>数据库的隔离级别实现</h4><p>那么，这几种隔离级别怎么实现的呢？我们可以先试想一下，如果用读写锁来实现，要怎么加锁才能达到效果？</p><p><strong>注意：</strong>这里是假设用锁来实现，MySQL 的实现原理实际并不是这样的，下面我们会谈到。</p><h5 id="脏读问题"><a href="#脏读问题" class="headerlink" title="脏读问题"></a>脏读问题</h5><p>事务1更新了一条数据，但是还没提交，此时事务2读取了这条数据，然后事务1因为某些原因进行了回滚，此时事务2读取到的数据其实是不对的，读到了脏数据就称之为脏读。（注：这就是读未提交隔离级别，能读取到未提交的数据，会发生脏读。读未提交不需要解析实现原理了吧，啥都没限制，直接读直接写就是了）</p><p>那么，我们怎么避免脏读？</p><p>如果用锁来实现的话，我们先来做个约定：更新修改删除数据要加上写锁，读取数据要加上读锁，读锁和写锁是互斥的，读锁和读锁是不互斥的。读锁读完数据就释放，写锁事务提交释放。</p><p>有了这个约定，我们就可以这样来避免脏读了：修改数据我们先加上写锁，读取数据要加上读锁，因为事务1还没提交，写锁还没有释放，此时事务2没法加上读锁，这样就读不到数据，避免了脏读。（注：这就是读已提交隔离级别用锁来实现的原理）</p><h5 id="不可重复读问题"><a href="#不可重复读问题" class="headerlink" title="不可重复读问题"></a>不可重复读问题</h5><p>事务1读取了一条数据，此时事务2更新了这条数据，然后事务1再读取这条数据，发现自己两次读取的内容有点不一样，这就是不可重复读。（注：读已提交隔离级别用锁来实现虽然解决了脏读，但是会存在不可以重复读）</p><p>那么，我们怎么避免不可重复读？</p><p>我们回想一下发送不可重复读的原因，正是因为我们之前设定读完数据就释放读锁，所以事务2才加上了写锁对数据进行了修改，如果事务1第一次读完数据不释放读锁，事务2就加不上写锁，就不会出现这个问题了。好吧，那读锁也事务提交的时候再释放吧，这样就不会发生不可重复读问题了。（注：这就是可重复读用锁来实现的原理了）</p><h5 id="幻读问题"><a href="#幻读问题" class="headerlink" title="幻读问题"></a>幻读问题</h5><p>事务1根据查询条件读取了若干条数据，事务2插入了符合事务1查询条件的数据，此时事务1再查一次，发现记录竟然多了一条，这就是幻读。（注：用锁来实现的可重复读虽然解决了不可重复读的问题，但是会存在幻读，因为新插入的数据并没有锁定）</p><p>那么，我们怎么避免幻读问题？</p><p>既然新插入数据导致我幻读，那我就想办法让他不能插入就行了。我们不仅把符合查询条件的记录锁起来，还把记录前后的 GAP 锁起来，这样其他事务就无法插入新数据了。（注：这就是序列化隔离级别了，你回想一下，这其实就是我在干活的时候，其他事务任何操作都做不了了，只能等我干完了才行）</p><h4 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h4><p>实际上, MySQL 并不会这样实现，因为锁就是性能杀手。在 MySQL InnoDB 存储引擎中，使用的是基于多版本的并发控制协议——MVCC (Multi-Version Concurrency Control) (注：与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control，类似我们上面的推导)。</p><p>MVCC最大的好处：<strong>读不加锁，读写不冲突</strong>。</p><p>注意：在 MySQL InnoDB 存储引擎，MVCC 只用于 RC,RR 两个隔离级别。序列化隔离级别跟我们上面的推导是一样的，读加读锁，写加写锁，读写互斥，性能杀手。</p><p>在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。</p><p>那啥是快照读，啥是当前读？以MySQL InnoDB为例：</p><ul><li><strong>快照读</strong>：简单的select操作，属于快照读，不加锁。(当然，也有例外，下面会分析)<br>  select * from table where ?;</li></ul><ul><li><strong>当前读</strong>：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。<br>  select <em> from table where ? lock in share mode;<br>  select </em> from table where ? for update;<br>  insert into table values (…);<br>  update table set ? where ?;<br>  delete from table where ?;</li></ul><p>所有以上的语句，都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。</p><p>对于 MVCC 的实现原理就不展开分析了，可以参考文末的引用链接。</p><p>以上就回答了文章开头的第二个问题。</p><h4 id="隔离级别与锁的关系"><a href="#隔离级别与锁的关系" class="headerlink" title="隔离级别与锁的关系"></a>隔离级别与锁的关系</h4><p>从上面的分析来看，隔离级别有用到锁。那么我们在平时写业务代码的时候还需要手工加锁吗？</p><p>我们试想一个购票的业务方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">1. 查询余票（select 余票 from xxxx）</span><br><span class="line">2. 余票扣减(update xxxx set 余票=余票-1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上文谈到，隔离级别是为了在并发的情况下读取数据的正确性。那么现在有两个事务，几乎同时走到了步骤1查询余票（没有显式使用锁是快照度），都还没有走到步骤2。大家查询出的余票都是1，数据正确的。当事务1提交，余票变成0。当事物2提交（没有使用乐观锁），余票变成-1，出问题了，一张票卖了两次。</p><p>所以，在这种情况下，我们需要显式使用到锁或者其他方式解决业务并发的问题。隔离级别没法帮我们解决这种并发问题。</p><h4 id="MVCC-疑问"><a href="#MVCC-疑问" class="headerlink" title="MVCC 疑问"></a>MVCC 疑问</h4><p>既然采用了 MVCC ，那么 RC 隔离级别下读取的应该是快照，为什么没有解决不可重复读呢？因为 RC 在每次读取的时候都会重建 read view ，其他事务修改的数据在第二次读取的时重建 read view 是能读取到的，所以就出现了不可重复读问题。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>以上只是我个人不断的自问自答方式来解惑自己的问题，思路有点乱。但是，通过这种不断自问自答的方式刨根问底，很多问题就逐渐明朗起来了。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li><a href="https://tech.meituan.com/2014/08/20/innodb-lock.html" target="_blank" rel="noopener">Innodb中的事务隔离级别和锁的关系</a></li><li><a href="http://blog.sae.sina.com.cn/archives/2127" target="_blank" rel="noopener">MySQL 加锁处理分析</a></li><li><a href="https://elsef.com/2019/03/10/MySQL%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%90%84%E7%A7%8D%E4%B8%8D%E6%AD%A3%E5%B8%B8%E8%AF%BB/" target="_blank" rel="noopener">MySQL的MVCC在各种隔离级别中发挥的作用</a></li><li><a href="https://mp.weixin.qq.com/s/bM_g6Z0K93DNFycvfJIbwQ" target="_blank" rel="noopener">数据库村的旺财和小强</a></li><li><a href="http://mysql.taobao.org/monthly/2018/03/01/" target="_blank" rel="noopener">数据库内核月报</a></li><li><a href="https://ningyu1.github.io/site/post/50-mysql-gap-lock/" target="_blank" rel="noopener">MySQL Gap Lock问题</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在工作中，碰到一些问题的时候你会想，诶，这个知识点我似乎没理解透。数据库的隔离级别就是我最近有点困惑的知识点，大家试着回答以下问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;数据库的隔离级别究竟是为什么解决什么出题而出现的？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;数据库的隔离级别是怎么
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Transactional 注解与 AOP</title>
    <link href="http://ideajava.com/2019/08/29/Transactional-and-Aop/"/>
    <id>http://ideajava.com/2019/08/29/Transactional-and-Aop/</id>
    <published>2019-08-29T12:18:49.000Z</published>
    <updated>2019-09-20T07:04:57.600Z</updated>
    
    <content type="html"><![CDATA[<p>当我们在一个 private 方法上打上 @Transactional 注解，IDEA 会提示 <code>Methods annotated with &#39;@Transactional&#39; must be overridable</code>。比如下面的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Service</span><br><span class="line">public class UserService &#123;</span><br><span class="line"></span><br><span class="line">    public void method1()&#123;</span><br><span class="line">        method2();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Transactional</span><br><span class="line">    private void method2()&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有时候我们想让事务的范围尽可能的小，可能就会写出这样的代码，在 method1 先做一些非事务的事情，然后在 method2 中做事务相关的事情。此时，如果我们用 IDEA 的话，应该就会出现上面的提示信息，eclipse 我没用不清楚会不会有这样的其实。为什么被 @Transactional 注解的方法必须是可重写的呢？</p><p>如果我们忽略掉这个信息，你会发现编译运行是没有问题的。在外部调用 UserService.method1() 时，如果 method2 没有发生异常的话，你会发现一切都是正常的。为什么 IDEA 要这样提示呢，真是怪了！</p><p>但是，当 method2 发生异常时，你会惊奇的发现 method2 的事务并<strong>没有回滚！</strong></p><h3 id="AOP"><a href="#AOP" class="headerlink" title="AOP"></a>AOP</h3><p>要理解事务为什么没有回滚，我们就要回顾一下 AOP 的知识。在 Spring 中默认是通过 JDK 动态代理的方式来实现 AOP 的，对于打了 @Transactional 注解的类，Spring 动态代理会生成一个代理 bean 和一个真实的目标 bean。</p><p>我们回头看看 method1 并没有打上注解，所以 method1 并不会被事务切面环绕。而 method2 是通过 method1 调用的，隐藏的调用对象是真实的目标 bean，真实的目标 bean 是没有切面逻辑的，切面逻辑都在代理 bean 上。这就是为什么事务没有回滚的原因，如下图所示：</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/transactional-aop.gif" alt="transactional-aop"></p><p>按照这样的理解，只要我们在 method1 方法上打上 @Transactional 注解，事务就能生效了，method2 的注解是多余的。此时，我们应该就能理解 IDEA 的善意提示了。因为 UserService 没有接口，所以只能通过 CGLIB 的方式来实现动态代理，而 CGLIB 是通过继承的方式来进行代理，需要对目标 bean 的方法进行重写，但是 private 修饰的方法是不能重写的，所以就会出现这样的提示。</p><p>但是，在 method1 上打注解，method2 上不打，那不是违背了我们当初让事务范围最小化的出发点了吗？一切又回到了原地。还有没有其他解决方法？</p><ol><li>把 method1 方法搬到 controller 层，嘿嘿，这个方法看起来很鸡贼，但是好像是更加合理的？</li><li>如果代理类和目标类合为一个，有注解的方法有切面环绕，没有注解的方法没有切面环绕，这样是不是就行了？AspectJ 静态代理就是这样的一门技术，在编译成 class 字节码的时候在方法周围织入切面逻辑。</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>不仅仅是 @Transactional 会这样，其他的注解也是同理的，所以我们要深入理解 AOP 的原理，面对这些问题才能游刃有余。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当我们在一个 private 方法上打上 @Transactional 注解，IDEA 会提示 &lt;code&gt;Methods annotated with &amp;#39;@Transactional&amp;#39; must be overridable&lt;/code&gt;。比如下面的例子：
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Spring transaction propagation</title>
    <link href="http://ideajava.com/2019/08/22/Spring-transaction-propagation/"/>
    <id>http://ideajava.com/2019/08/22/Spring-transaction-propagation/</id>
    <published>2019-08-22T06:06:55.000Z</published>
    <updated>2019-08-22T07:42:34.498Z</updated>
    
    <content type="html"><![CDATA[<p>有些简单的知识点理解的比较含糊，在使用的时候总是会卡壳，Spring 事务的传播行为就是一个例子。今天再次复习一下。</p><p>事务的传播行为指的是两个方法调用之间事务怎么传播的问题。比如，方法A 有事务，调用的方法B也有事务，那么方法B 的事务是直接加入方法A 的事务还是让方法A 的事务挂起呢？当然，Spring 的事务传播行为不仅仅只有这两种情况。</p><h3 id="事务传播属性"><a href="#事务传播属性" class="headerlink" title="事务传播属性"></a>事务传播属性</h3><ul><li><strong>PROPAGATION_REQUIRED：</strong> 如果方法A没有事务，就新建一个事务；如果有，就加入当前事务。这是 Spring 提供的默认事务传播行为。</li><li><strong>PROPAGATION_SUPPORTS：</strong> 如果方法A没有事务，就以非事务方式执行；如果有，就使用当前事务。</li><li><strong>PROPAGATION_MANDATORY：</strong> 如果方法A没有事务，就抛出异常；如果有，就使用当前事务。</li><li><strong>PROPAGATION_REQUIRES_NEW：</strong> 如果方法A没有事务，就新建一个事务；如果有，就将当前事务挂起。</li><li><strong>PROPAGATION_NOT_SUPPORTED：</strong> 如果方法A没有事务，就以非事务方式执行；如果有，就将当前事务挂起。</li><li><strong>PROPAGATION_NEVER：</strong> 如果方法A没有事务，就以非事务方式执行；如果有，就抛出异常。</li><li><strong>PROPAGATION_NESTED：</strong> 如果方法A没有事务，就新建一个事务；如果有，就作为当前事务的嵌套事务。</li></ul><p>前六个应该都很好理解，就是最后一个传播行为可能会比较难理解，很容易和 PROPAGATION_REQUIRES_NEW 混淆，下面我们来分析一下这两种行为的区别。</p><h3 id="PROPAGATION-NESTED-与-PROPAGATION-REQUIRES-NEW-的区别"><a href="#PROPAGATION-NESTED-与-PROPAGATION-REQUIRES-NEW-的区别" class="headerlink" title="PROPAGATION_NESTED 与 PROPAGATION_REQUIRES_NEW 的区别"></a>PROPAGATION_NESTED 与 PROPAGATION_REQUIRES_NEW 的区别</h3><p>PROPAGATION_REQUIRES_NEW 是新开一个事务，是独立的，不会受外部事务的影响。当新开的事务开始执行时，外部事务会被挂起，内部事务结束了，外部事务继续执行。<br><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/transaction-propagation-1.png" alt="PROPAGATION_REQUIRES_NEW"></p><p>PROPAGATION_NESTED 是一个 “嵌套的” 事务，它是已经存在事务的一个真正的子事务。嵌套事务开始执行时，它将取得一个 savepoint。如果这个嵌套事务失败，我们将回滚到此 savepoint。本质上，外部事务和嵌套事务属于同一个物理事务，使用保存点实现嵌套事务。嵌套事务和它的父事务是相依的，它的提交要和它的父事务一起。也就是说，如果父事务最后回滚，它也要回滚。如果子事务回滚或提交，不会导致父事务回滚或提交，但父事务回滚将导致子事务回滚。<br><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/transaction-propagation-2.png" alt="PROPAGATION_NESTED"></p><h3 id="PROPAGATION-NESTED-用法示例"><a href="#PROPAGATION-NESTED-用法示例" class="headerlink" title="PROPAGATION_NESTED 用法示例"></a>PROPAGATION_NESTED 用法示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ServiceA &#123;  </span><br><span class="line">      </span><br><span class="line">    /** </span><br><span class="line">     * 事务属性配置为 PROPAGATION_REQUIRED </span><br><span class="line">     */  </span><br><span class="line">    void methodA() &#123;  </span><br><span class="line">        try &#123;  </span><br><span class="line">            ServiceB.methodB();  </span><br><span class="line">        &#125; catch (SomeException) &#123;  </span><br><span class="line">            // 执行其他业务, 如 ServiceC.methodC();  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种方式也是嵌套事务最有价值的地方，它起到了分支执行的效果, 如果 ServiceB.methodB 失败, 那么执行 ServiceC.methodC(), 而 ServiceB.methodB 已经回滚到它执行之前的 SavePoint, 所以不会产生脏数据(相当于此方法从未执行过), 这种特性可以用在某些特殊的业务中, 而 PROPAGATION_REQUIRED 和 PROPAGATION_REQUIRES_NEW 都没有办法做到这一点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;有些简单的知识点理解的比较含糊，在使用的时候总是会卡壳，Spring 事务的传播行为就是一个例子。今天再次复习一下。&lt;/p&gt;
&lt;p&gt;事务的传播行为指的是两个方法调用之间事务怎么传播的问题。比如，方法A 有事务，调用的方法B也有事务，那么方法B 的事务是直接加入方法A 的事务
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>circuit-breaker</title>
    <link href="http://ideajava.com/2019/08/08/circuit-breaker/"/>
    <id>http://ideajava.com/2019/08/08/circuit-breaker/</id>
    <published>2019-08-08T08:17:29.000Z</published>
    <updated>2019-08-12T04:06:57.132Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式系统中，对远程服务的调用很可能会<strong>临时性</strong>失败（如网络连接超时或资源过载等）。这些故障通常会在短时间内自我修复，通常我们在搭建一个可靠的分布式系统时，会通过<strong>重试</strong>策略来应对这些问题。</p><p>但是，也有可能会遇到一些意外事件导致的故障，这种故障可能会需要比较长的时间才能修复。在这种情况下，应用程序重试多少次都是失败的，而且如果此时并发很大，这些毫无意义的重试很有可能把整个服务拖垮。所以，在这种情况下，应该是让应用程序快速认识到操作已失败，并相应地处理故障。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p><strong>断路器</strong>模式可以防止应用程序重复尝试执行很可能会失败的操作。在发生故障期间，让它快速失败进入兜底逻辑，不用浪费 CPU 周期。 断路器模式还可让应用程序检测故障是否已经解决。 如果问题已被修复，应用程序便可以尝试调用操作。</p><blockquote><p>断路器模式的目的与重试模式不同。 重试模式在预期操作将成功的情况下让应用程序重试操作。 断路器模式则防止应用程序执行很可能失败的操作。 应用程序可以使用重试模式通过断路器调用操作，来组合这两种模式。 但重试逻辑应该对断路器返回的任何异常保持敏感，并且在断路器指示故障为非临时性的情况下放弃重试尝试。</p></blockquote><p>针对可能失败的操作，断路器充当其代理。代理监控最近发生失败的次数，并使用此信息来决定是执行正常的逻辑还是兜底逻辑或者是直接返回异常。</p><h3 id="断路器状态机"><a href="#断路器状态机" class="headerlink" title="断路器状态机"></a>断路器状态机</h3><ul><li><p><strong>关闭：</strong>将应用程序的请求路由到正常逻辑。代理维护最近失败次数的计数，如果对操作的调用不成功，代理将递增此计数。 如果在给定时间段内最近失败次数超过指定的阈值，则代理将置于<strong>打开</strong>状态。 此时，代理会启动超时计时器，并且当此计时器过期时，代理将置于<strong>半开</strong>状态。</p></li><li><p><strong>打开：</strong>来自应用程序的请求立即失败，并为应用程序返回异常或者执行兜底逻辑。</p></li><li><p><strong>半开：</strong>允许数量有限的来自应用程序的请求通过并调用操作。 如果这些请求成功，则假定先前导致失败的问题已被修复，并且断路器将切换到<strong>关闭</strong>状态（失败计数器重置）。 如果有任何请求失败，则断路器将假定故障仍然存在，因此它会恢复到<strong>打开</strong>状态，并重新启动超时计时器，再给系统一段时间来从故障中恢复。</p></li></ul><blockquote><p>半开状态对于防止恢复服务突然被大量请求压垮很有用。 在服务恢复的期间，能够支持的请求数量有限，如果突然有大量的请求可能导致服务超时或再次失败。</p></blockquote><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/circuit-breaker-diagram.png" alt="断路器状态机"></p><p>在图中，<strong>关闭</strong>状态所使用的失败计数器是<strong>基于时间</strong>的。 它会定期自动重置。 这有助于防止断路器在遇到偶然失败时进入<strong>打开</strong>状态。 仅当在指定间隔期间内发生指定数量的失败时，才会达到将断路器跳闸到<strong>打开</strong>状态的故障阈值。 <strong>半开</strong>状态使用的计数器记录成功调用操作的次数。 在指定数量的连续操作调用成功后，断路器将恢复到<strong>关闭</strong>状态。 如果任何调用失败，断路器会立即进入<strong>打开</strong>状态，成功计数器会在下次进入<strong>半开</strong>状态时重置。</p><p>断路器模式在系统从故障中恢复时提供稳定性，并将对性能的影响降至最低。 它可以通过快速拒绝很可能失败的操作的请求（而非等待操作超时或永不返回）来帮助维持系统的响应时间。 如果断路器在每次改变状态时引发事件，则该信息可以用于监视由断路器保护的系统部分的运行状况，或者当断路器跳闸到打开状态时，对管理员发出警报。</p><p>该模式是可自定义的，并且可以根据可能的故障类型进行调整。 例如，可以向断路器应用可递增的超时计时器。 最开始可以将断路器置于打开状态几秒钟，如果故障未得到解决，则将超时增加到几分钟，以此类推。 在某些情况下，与其通过打开状态返回失败并引发异常，返回对应用程序来说有意义的默认值实则更加有用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在分布式系统中，对远程服务的调用很可能会&lt;strong&gt;临时性&lt;/strong&gt;失败（如网络连接超时或资源过载等）。这些故障通常会在短时间内自我修复，通常我们在搭建一个可靠的分布式系统时，会通过&lt;strong&gt;重试&lt;/strong&gt;策略来应对这些问题。&lt;/p&gt;
&lt;p&gt;但是，
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Learning Ribbon</title>
    <link href="http://ideajava.com/2019/07/09/Learning-Ribbon/"/>
    <id>http://ideajava.com/2019/07/09/Learning-Ribbon/</id>
    <published>2019-07-09T11:48:59.000Z</published>
    <updated>2019-08-12T04:53:32.190Z</updated>
    
    <content type="html"><![CDATA[<p>研究一番 Eureka 之后, 就要顺带研究一下 Ribbon 了. 服务发现了, 怎么去做负载均衡呢, Ribbon 应运而生.</p><p>简单的来讲, 客户端负载均衡就是拿到一系列服务地址之后, 自己按照某种负载均衡算法去挑一个调用就完了. </p><p>下面我们来看下 Ribbon 怎么实现负载均衡.</p><h3 id="LoadBalanced"><a href="#LoadBalanced" class="headerlink" title="@LoadBalanced"></a>@LoadBalanced</h3><p>假设我们使用 <code>RestTemplate</code> 作为 HTTP Client.</p><p>首先我们要在 classpath 中引入 <code>spring-cloud-starter-netflix-ribbon</code> 依赖, 然后在 <code>RestTemplate</code> 上标记 <code>@LoadBalanced</code> 注解, 这样使用 <code>RestTemplate</code> 进行访问时就可以实现负载均衡了.</p><h3 id="LoadBalancerAutoConfiguration"><a href="#LoadBalancerAutoConfiguration" class="headerlink" title="LoadBalancerAutoConfiguration"></a>LoadBalancerAutoConfiguration</h3><p>Ribbon 究竟是怎么做到打了个注解就可能实现负载均衡了呢? 核心就在于 <code>LoadBalancerAutoConfiguration</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">@ConditionalOnClass(RestTemplate.class)</span><br><span class="line">@ConditionalOnBean(LoadBalancerClient.class)</span><br><span class="line">@EnableConfigurationProperties(LoadBalancerRetryProperties.class)</span><br><span class="line">public class LoadBalancerAutoConfiguration &#123;</span><br><span class="line"></span><br><span class="line">@LoadBalanced</span><br><span class="line">@Autowired(required = false)</span><br><span class="line">private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList();</span><br><span class="line"></span><br><span class="line">@Bean</span><br><span class="line">public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated(</span><br><span class="line">final ObjectProvider&lt;List&lt;RestTemplateCustomizer&gt;&gt; restTemplateCustomizers) &#123;</span><br><span class="line">return () -&gt; restTemplateCustomizers.ifAvailable(customizers -&gt; &#123;</span><br><span class="line">            for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) &#123;</span><br><span class="line">                for (RestTemplateCustomizer customizer : customizers) &#123;</span><br><span class="line">                    customizer.customize(restTemplate);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码有两个核心作用:</p><ul><li>将 classpath 中被标记了 <code>@LoadBalanced</code> 注解的 <code>RestTemplate</code> 全部注入到 <code>restTemplates</code> 列表中.</li><li>将 <code>restTemplates</code> 列表中每个 <code>RestTemplate</code> 通过 <code>RestTemplateCustomizer</code> 进行了自定义.</li></ul><p>RestTemplateCustomizer 究竟进行了什么自定义, 继续看下面的代码片段:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">@ConditionalOnMissingClass(&quot;org.springframework.retry.support.RetryTemplate&quot;)</span><br><span class="line">static class LoadBalancerInterceptorConfig &#123;</span><br><span class="line">@Bean</span><br><span class="line">public LoadBalancerInterceptor ribbonInterceptor(</span><br><span class="line">LoadBalancerClient loadBalancerClient,</span><br><span class="line">LoadBalancerRequestFactory requestFactory) &#123;</span><br><span class="line">return new LoadBalancerInterceptor(loadBalancerClient, requestFactory);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Bean</span><br><span class="line">@ConditionalOnMissingBean</span><br><span class="line">public RestTemplateCustomizer restTemplateCustomizer(</span><br><span class="line">final LoadBalancerInterceptor loadBalancerInterceptor) &#123;</span><br><span class="line">return restTemplate -&gt; &#123;</span><br><span class="line">               List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;(</span><br><span class="line">                       restTemplate.getInterceptors());</span><br><span class="line">               list.add(loadBalancerInterceptor);</span><br><span class="line">               restTemplate.setInterceptors(list);</span><br><span class="line">           &#125;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从这段代码可以看出, <code>RestTemplateCustomizer</code> 为每个 <code>RestTemplate</code> 添加了一个 <code>LoadBalancerInterceptor</code> 拦截器. 到这里, 我们应该大概能猜到 Ribbon 实现负载均衡的核心逻辑了. 就是通过拦截器将请求拦截下来, 然后获取要访问的服务名称, 根据服务名名称获取到服务列表, 最后根据某种负载均衡算法从列表中挑取一个进行调用.</p><p>当然, 这只是我们的猜想, 看看源码是不是.</p><h3 id="LoadBalancerInterceptor"><a href="#LoadBalancerInterceptor" class="headerlink" title="LoadBalancerInterceptor"></a>LoadBalancerInterceptor</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class LoadBalancerInterceptor implements ClientHttpRequestInterceptor &#123;</span><br><span class="line"></span><br><span class="line">private LoadBalancerClient loadBalancer;</span><br><span class="line">private LoadBalancerRequestFactory requestFactory;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public ClientHttpResponse intercept(final HttpRequest request, final byte[] body,</span><br><span class="line">final ClientHttpRequestExecution execution) throws IOException &#123;</span><br><span class="line">final URI originalUri = request.getURI();</span><br><span class="line">String serviceName = originalUri.getHost();</span><br><span class="line">Assert.state(serviceName != null, &quot;Request URI does not contain a valid hostname: &quot; + originalUri);</span><br><span class="line">return this.loadBalancer.execute(serviceName, requestFactory.createRequest(request, body, execution));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个拦截器的代码比较简单, 只是从请求 URI 中获取到了服务名称, 然后调用负载均衡器 LoadBalancerClient 的 execute 方法, 这跟我们上面的设想差不多.</p><h3 id="RibbonLoadBalancerClient"><a href="#RibbonLoadBalancerClient" class="headerlink" title="RibbonLoadBalancerClient"></a>RibbonLoadBalancerClient</h3><p>LoadBalancerClient 的实现类是 RibbonLoadBalancerClient, 我们来看下他的 execute 方法.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request) throws IOException &#123;</span><br><span class="line">ILoadBalancer loadBalancer = getLoadBalancer(serviceId);</span><br><span class="line">Server server = getServer(loadBalancer);</span><br><span class="line">if (server == null) &#123;</span><br><span class="line">throw new IllegalStateException(&quot;No instances available for &quot; + serviceId);</span><br><span class="line">&#125;</span><br><span class="line">RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server,</span><br><span class="line">serviceId), serverIntrospector(serviceId).getMetadata(server));</span><br><span class="line"></span><br><span class="line">return execute(serviceId, ribbonServer, request);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>先根据 serviceId 获取到 ILoadBalancer 实例, 跟踪代码会发现每一个 serviceId 都有一个对应的 spring 上下文, 找出属于自己的 ILoadBalancer. ILoadBalancer 是 Ribbon 的核心类。可以理解成它包含了选取服务的规则(IRule)、服务集群的列表(ServerList)、检验服务是否存活(IPing)等特性，同时它也具有了根据这些特性从服务集群中选取具体一个服务的能力。</li><li>通过 loadBalancer 实例根据规则选择出相应的 Server</li><li>执行 execute 方法</li></ol><p>现在, 我们再看下 execute 方法内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public &lt;T&gt; T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest&lt;T&gt; request) throws IOException &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">T returnVal = request.apply(serviceInstance);</span><br><span class="line">statsRecorder.recordStats(returnVal);</span><br><span class="line">return returnVal;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意, 上面的代码我简化掉了, 只留下了我们需要关注的核心逻辑. 实际上就是调用了 request.apply(serviceInstance) 方法, 就是将请求应用于挑选出来的那个服务系统.</p><h3 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h3><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/ribbon-loadbalancer.png" alt="Ribbon"></p><p>本文只是简单梳理了 Ribbon 实现负载均衡的核心流程, 但是具体的负载均衡算法没有去深入考究, 也就是 ILoadBalancer 里面的方法实现, 涉及到 IRule, IRule, ServerList, 有兴趣的可以继续深入研究.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;研究一番 Eureka 之后, 就要顺带研究一下 Ribbon 了. 服务发现了, 怎么去做负载均衡呢, Ribbon 应运而生.&lt;/p&gt;
&lt;p&gt;简单的来讲, 客户端负载均衡就是拿到一系列服务地址之后, 自己按照某种负载均衡算法去挑一个调用就完了. &lt;/p&gt;
&lt;p&gt;下面我们
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Eureka Architecture</title>
    <link href="http://ideajava.com/2019/07/04/Eureka-Architecture/"/>
    <id>http://ideajava.com/2019/07/04/Eureka-Architecture/</id>
    <published>2019-07-04T05:52:41.000Z</published>
    <updated>2019-08-12T04:06:53.280Z</updated>
    
    <content type="html"><![CDATA[<p>被誉为下一代微服务架构的 Service Mesh 已经越来越火了, 我还在写 Eureka 的文章, 是不是太 low 了. 那就先 low 一回, 下次再时尚一把.</p><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/eureka_architecture.png" alt="Eureka Architecture"></p><p>上图是 Eureka 官方给出的架构图, 一共有三种角色:</p><ul><li>Eureka Server: Eureka 服务端, 核心就是保存服务的注册表信息, 如果是集群部署, 多个服务端之间会复制注册表信息, 达到最终的一致性.</li><li>Application Service: 即 Service Provider , 作为 Eureka 的客户端, 启动时会注册到 Eureka 服务端, 对外提供服务.</li><li>Application Client: 即 Server Consumer , 也是 Eureka 客户端, 启动时会用 Eureka 服务端获取所有的服务列表.</li></ul><h4 id="Eureka-Server"><a href="#Eureka-Server" class="headerlink" title="Eureka Server"></a>Eureka Server</h4><p>对于 Eureka 服务端, 我们首先能想到的几个问题:</p><ol><li>服务提供者的注册过程是怎么样的, 用什么样的数据结构来保存服务提供者信息?</li><li>Eureka 服务端怎么知道服务提供者挂掉了, 挂掉了之后会怎么办?</li><li>集群情况下, Eureka 服务端之间的注册表信息是怎么同步的?</li><li>集群情况下, 一个 Eureka 服务端启动的时候, 是怎么获取到其他节点的注册表信息的?</li></ol><h5 id="服务注册-Register"><a href="#服务注册-Register" class="headerlink" title="服务注册(Register)"></a>服务注册(Register)</h5><p>Eureka 服务端是通过暴露 HTTP 接口来提供服务注册的; 另外, 更新服务状态也是这个接口, 比如服务下线, 会调用这个接口将状态更新为 DOWN.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.netflix.eureka.resources.ApplicationResource#addInstance</span><br></pre></td></tr></table></figure><p>注册的过程比较简单, 先是注册到本节点, 然后调用 replicateToPeers 向其它 Eureka Server 节点做状态同步（异步操作）</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/service-register.png" alt="Service Register"></p><p>注册的服务信息保存在一个嵌套的 HashMap 中:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConcurrentHashMap&lt;String, Map&lt;String, Lease&lt;InstanceInfo&gt;&gt;&gt; registry</span><br></pre></td></tr></table></figure><ul><li>第一层 HashMap 的 key 是 appName，也就是应用名字</li><li>第二层 HashMap 的 key 是 instanceId ，也就是实例ID</li></ul><p>为什么是这样的结构, 大家想一下应该就知道了, 因为一个应用会存在多个实例.</p><h5 id="服务续约-Renew"><a href="#服务续约-Renew" class="headerlink" title="服务续约(Renew)"></a>服务续约(Renew)</h5><p>Eureka 服务端会提供服务续约接口给服务提供者调用, 让它告诉 Eureka 服务端它还活着, 正常提供者服务呢, 不要把我给踢掉.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.netflix.eureka.resources.InstanceResource#renewLease</span><br></pre></td></tr></table></figure><p>接口的实现跟 Register 基本保持一致, 先更新自身节点状态, 然后同步到其他节点.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/service-renew.png" alt="Service Renew"></p><h5 id="服务下线-Cancel"><a href="#服务下线-Cancel" class="headerlink" title="服务下线(Cancel)"></a>服务下线(Cancel)</h5><p>Eureka 服务端会提供服务下线接口给服务提供者调用, 让它告诉 Eureka 服务端它要下线了, 不对外提供服务了, 把我踢掉吧.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.netflix.eureka.resources.InstanceResource#cancelLease</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/service-cancel.png" alt="Service Cancel"></p><h5 id="服务获取"><a href="#服务获取" class="headerlink" title="服务获取"></a>服务获取</h5><p>Eureka 服务端会提供接口让服务消费者调用, 用来获取注册在 Eureka 服务端上的服务.</p><p>为了提高性能，服务列表在Eureka Server会缓存一份，同时每30秒更新一次.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/fetch-service.png" alt="Service Fetch"></p><h5 id="服务剔除"><a href="#服务剔除" class="headerlink" title="服务剔除"></a>服务剔除</h5><p>Eureka 服务端会定时检查在一定时间内(默认是90秒)没有续约的服务, 默认是每60秒检查一次. 也就是如果有服务超过90秒没有向 Eureka 服务端发起续约请求的话，就会被当做失效服务剔除掉(如果没有开启自我保护模式的话). 自我保护模式这里不详细展开, 有兴趣的可以谷歌查一下.</p><p>失效时间可以通过 <code>eureka.instance.leaseExpirationDurationInSeconds</code> 进行配置，定期扫描时间可以通过 <code>eureka.server.evictionIntervalTimerInMs</code> 进行配置.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/service-eviction.png" alt="Service Eviction"></p><h5 id="节点间服务同步"><a href="#节点间服务同步" class="headerlink" title="节点间服务同步"></a>节点间服务同步</h5><p>上面的 Register、Renew、Cancel 接口都会先在本节点进行操作, 然后同步到其他节点. 同步的方式也很简单, 就是将服务提供者请求的方式一样转发到其他节点.</p><p>节点之间的状态是采用异步的方式同步的，所以不保证节点间的状态一定是一致的，不过基本能保证最终状态是一致的.</p><p>结合服务发现的场景，实际上也并不需要节点间的状态强一致。在一段时间内（比如30秒），节点A比节点B多一个服务实例或少一个服务实例，在业务上也是完全可以接受的（Service Consumer侧一般也会实现错误重试和负载均衡机制）。</p><h5 id="节点互相感知"><a href="#节点互相感知" class="headerlink" title="节点互相感知"></a>节点互相感知</h5><p>Eureka 服务端之间需要同步注册表的前提是知道对方的存在, 那么他们之间是怎么互相感知的?</p><p>Eureka Server在启动后会调用 <code>EurekaClientConfig.getEurekaServerServiceUrls</code> 来获取所有的节点，并且会定期更新。定期更新频率可以通过 <code>eureka.server.peerEurekaNodesUpdateIntervalMs</code> 配置。</p><p>默认情况下是从 <code>eureka.client.serviceUrl</code> 配置项读取其他节点的地址, 如果希望灵活控制节点, 可以重写 <code>getEurekaServerServiceUrls</code> 方法从外部存储读取服务端地址列表.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/peer-discovery.png" alt="Peer Discovery"></p><h5 id="节点初始化"><a href="#节点初始化" class="headerlink" title="节点初始化"></a>节点初始化</h5><p>一个新的 Eureka 服务端节点启动时, 会从其他节点获取到所有的服务信息, 前提是 <code>register-with-eureka = true</code>, 当然这个配置项默认就是 true.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/server-init.png" alt="Server Init"></p><h4 id="Service-Provider"><a href="#Service-Provider" class="headerlink" title="Service Provider"></a>Service Provider</h4><p>上面说的是 Eureka 服务端的工作机制, 接下来就是 Eureka 客户端, 首先看下服务提供者.</p><p>对于服务的提供者, 主要就是 Register、Renew、Cancel 三个操作.</p><h5 id="服务注册-Register-1"><a href="#服务注册-Register-1" class="headerlink" title="服务注册(Register)"></a>服务注册(Register)</h5><p>将服务注册到 Eureka Server 上, 首先需要配置 <code>eureka.client.registerWithEureka=true</code>. 注册过程很简单, 就是在服务启动的时候会有一个定时任务调用 Eureka Server 提供的注册接口.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/client-register.png" alt="Client Register"></p><h5 id="服务续约-Renew-1"><a href="#服务续约-Renew-1" class="headerlink" title="服务续约(Renew)"></a>服务续约(Renew)</h5><p>服务提供者需要不断地告诉 Eureka Server 自己还活着, 现在这是一个定时任务不断地发送续约请求.有两个参数需要配置:</p><ul><li><code>instance.leaseRenewalIntervalInSeconds</code> 续约的频率, 就是多久发送一次续约请求, 默认 30 秒.</li><li><code>instance.leaseExpirationDurationInSeconds</code> 服务时效时间, 就是多少秒内没有发送需求请求, Eureka Server 就认为服务提供者挂了.</li></ul><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/client-renew.png" alt="Client Renew"></p><h5 id="服务下线-Cancel-1"><a href="#服务下线-Cancel-1" class="headerlink" title="服务下线(Cancel)"></a>服务下线(Cancel)</h5><p>在服务提供者 shut down 的时候，需要及时通知 Eureka Server 把自己剔除，从而避免客户端调用已经下线的服务。</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/client-cancel.png" alt="Client Cancel"></p><h5 id="怎么发现-Eureka-Server"><a href="#怎么发现-Eureka-Server" class="headerlink" title="怎么发现 Eureka Server"></a>怎么发现 Eureka Server</h5><p>服务提供者怎么知道 Eureka Server 的地址呢? 这个跟 Eureka Server 各个节点之间的发现是类似的, 默认也是从配置文件中读取, 灵活的话就是重写 <code>getEurekaServerServiceUrls</code> 方法.定期更新频率可以通过 <code>eureka.client.eurekaServiceUrlPollIntervalSeconds</code> 配置.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/client-discovery-server.png" alt="Client Discovery Server"></p><h4 id="Server-Consumer"><a href="#Server-Consumer" class="headerlink" title="Server Consumer"></a>Server Consumer</h4><p>服务消费者比较简单, 主要是怎么获取服务列表和更新服务列表.</p><h5 id="服务获取-1"><a href="#服务获取-1" class="headerlink" title="服务获取"></a>服务获取</h5><p>Service Consumer 在启动时会从 Eureka Server 获取所有服务列表，并在本地 <strong>缓存</strong> . 需要注意的是，需要确保配置 <code>eureka.client.shouldFetchRegistry=true</code>.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/client-fetch-service.png" alt="Client Fetch Service"></p><h5 id="服务更新"><a href="#服务更新" class="headerlink" title="服务更新"></a>服务更新</h5><p>由于在本地有一份缓存，所以需要定期更新，定期更新频率可以通过 <code>eureka.client.registryFetchIntervalSeconds</code> 配置.</p><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/client-update-service.png" alt="Client Update Service"></p><h5 id="怎么发现-Eureka-Server-1"><a href="#怎么发现-Eureka-Server-1" class="headerlink" title="怎么发现 Eureka Server"></a>怎么发现 Eureka Server</h5><p>服务消费者和服务提供者本质上都是 Eureka 的客户端, 所以逻辑是一样的.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>整个流程, 有很多定时任务, 这里梳理一下:</p><ul><li>Eureka Server: 定时检查更新服务列表缓存, 默认 30 秒; 定时剔除失效服务, 默认 60 秒; 定时更新其他服务端节点地址, 默认 10 分钟.</li><li>Service Provider: 定时续约, 默认 30 秒; 定时更新 Eureka Server 地址, 默认 5 分钟.</li><li>Server Consumer: 定时更新服务列表缓存, 默认 30 秒; 定时更新 Eureka Server 地址, 默认 5 分钟.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;被誉为下一代微服务架构的 Service Mesh 已经越来越火了, 我还在写 Eureka 的文章, 是不是太 low 了. 那就先 low 一回, 下次再时尚一把.&lt;/p&gt;
&lt;h4 id=&quot;总体架构&quot;&gt;&lt;a href=&quot;#总体架构&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Spring Kafka 的思考</title>
    <link href="http://ideajava.com/2019/07/02/Spring-Kafka/"/>
    <id>http://ideajava.com/2019/07/02/Spring-Kafka/</id>
    <published>2019-07-02T06:58:06.000Z</published>
    <updated>2019-07-02T09:41:04.450Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka 在高吞吐量的场景应用比较广泛, 但是可能会存在数据丢失, 那么 Kafka 究竟有多可靠? 本人只是简单实用过 Spring 集成的 Kafka, 并不是很熟悉, 所以下面思考的内容可能会有错.</p><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><h5 id="1-我怎么知道发送到-Broker-的消息发送成功了"><a href="#1-我怎么知道发送到-Broker-的消息发送成功了" class="headerlink" title="1.我怎么知道发送到 Broker 的消息发送成功了?"></a>1.我怎么知道发送到 Broker 的消息发送成功了?</h5><p>Spring 中的 <code>KafkaTemplate</code> 的发送方法返回 <code>ListenableFuture</code> 可以设置回调, 发送成功或者失败都是有回调的.</p><p>如果发送失败了怎么办? 在回调的失败方式中打个日志就算了, 还是重新再发送, 如果一直失败会不会死循环了? </p><p>目前我还没遇到发送失败的情况, 可能是样本还不够大, 这个问题还要深入思考.</p><p>另外, Spring Kafka 中有个配置项 <code>spring.kafka.producer.retries</code> 可以设置 <strong>重试</strong> 的次数, 这个是使用生产者内部的重试机制. 比如 Broker 返回 <code>LEADER_NOT_AVAILABLE</code> 错误, 生产者会自动进行重试. 但是重试可能会导致消息的重复, 在 <code>0.11.0.0</code> 版本之后, Kafka 提供了幂等的特性. 要不要用幂等的特性也是个问题, 不知道对吞吐量影响有多大, 所以一般情况还是让消费者来做幂等好了.</p><h5 id="2-发送成功了-Broker-还有没有可能丢失消息"><a href="#2-发送成功了-Broker-还有没有可能丢失消息" class="headerlink" title="2.发送成功了 Broker 还有没有可能丢失消息?"></a>2.发送成功了 Broker 还有没有可能丢失消息?</h5><p>如果 Broker 只有一份数据, 机器挂了, 是不是就丢失了? 当然 Kafka 消息是保存在硬盘中, 一般情况下重启之后消息还是存在的. 但是 Broker 在收到消息之后不是马上就刷盘的, 在刷盘之前挂了那消息就真的丢失了.</p><p>所以, 数据复制才是靠谱的选择, 挂了一份还有其他的. 然而, 数据复制就必然会遇到数据一致性的问题, Spring Kafka 给了我们几个发送确认选择:</p><ul><li><code>acks=0</code> 消息发送出去之后就认为写入成功了.</li><li><code>acks=1</code> 默认方式, Leader 节点写入后确认. </li><li><code>acks=all/-1</code> 所有节点写入后确认.</li></ul><p>显然, 这又是吞吐量与可靠性的一次 Trade Off.</p><h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><p>从本质上说, 保证了生产者到 Broker 之间的可靠, 那么 Kafka 其实就已经可靠了. 因为数据持久化在硬盘之后, 消费者可以根据情况控制偏移量来消费消息.</p><p>因此, 在消费者方面应该思考怎么刚好消费一次.</p><h5 id="1-什么情况下会出现消息重复消费"><a href="#1-什么情况下会出现消息重复消费" class="headerlink" title="1.什么情况下会出现消息重复消费?"></a>1.什么情况下会出现消息重复消费?</h5><p>当我们消费了消息, 但是偏移量没有提交, 那就会出现消息重复消息. 那么, 什么情况下会出现消费了消息, 但是偏移量没有被提交?</p><ul><li>消息被消费后, 提交偏移量之前, 进程挂了</li><li><code>0.10.0.0</code> 之前的版本,消费者没有单独的心跳进程, 如果到了 <code>session.timeout.ms</code> 时间, 获取的一批数据还没处理完, 会认为消费者挂了, 提交偏移量会失败</li></ul><h5 id="2-会不会出现消息还没有被消息-但是偏移量被提交了-导致消息-“丢失”"><a href="#2-会不会出现消息还没有被消息-但是偏移量被提交了-导致消息-“丢失”" class="headerlink" title="2.会不会出现消息还没有被消息, 但是偏移量被提交了, 导致消息 “丢失”?"></a>2.会不会出现消息还没有被消息, 但是偏移量被提交了, 导致消息 “丢失”?</h5><p>这里说的丢失指的是被跳过处理了. 因为只要消息被成功保存到 Broker 中,消费者可以通过偏移量指定消费哪条消息, 并没有真的丢失.</p><p>如果 <code>enable.auto.commit=true</code>, 当我们拉取到的消息使用另外的线程去处理, 但是另外的线程可能因其他原因没处理成功, 这种情况下就可能会出现消息”丢失”.</p><p>另外, 我们也可以想一下, 如果消息处理异常了, 但是还是自动提交了, 这种消息我们该怎么办? 重试, 记录日志或者是投递一个”死信队列”?</p><h4 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h4><h5 id="1-副本因子"><a href="#1-副本因子" class="headerlink" title="1.副本因子"></a>1.副本因子</h5><p>显然, Broker 要保证数据的可靠性就需要有副本, 并且最好不要在同一台机器上. <code>default.replication.factor</code> 用于设置副本数, 当副本数为 N 的时候, 可以容忍 N-1 个副本挂掉.</p><h5 id="2-脏副本的选举"><a href="#2-脏副本的选举" class="headerlink" title="2.脏副本的选举"></a>2.脏副本的选举</h5><p><code>unclean.leader.election.enable</code> 0.11.0.0 之前的版本, 默认为 true; 之后的版本默认为 false. 如果设置为 true, 当没有同步副本可用的时候, 不同步的副本会成为 leader, 意味着有数据丢失. 如果设置为 false, 则意味着系统会处于不可用的状态, 该部分没有 leader 提供服务. 需要在可用性和一致性之间做取舍.</p><h5 id="3-最小同步副本数"><a href="#3-最小同步副本数" class="headerlink" title="3.最小同步副本数"></a>3.最小同步副本数</h5><p><code>min.insync.replicas</code> 如果 broker 数为3, 最小同步副本数为2. 当2个同步副本中的一个出现问题, 集群便不会再接受生产者的发送消息请求. 同时客户端会收到 <code>NotEnoughReplicasException</code>. 此时, 消费者还可以继续读取存在的数据. 唯一的同步副本变成只读.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Kafka 在高吞吐量的场景应用比较广泛, 但是可能会存在数据丢失, 那么 Kafka 究竟有多可靠? 本人只是简单实用过 Spring 集成的 Kafka, 并不是很熟悉, 所以下面思考的内容可能会有错.&lt;/p&gt;
&lt;h4 id=&quot;生产者&quot;&gt;&lt;a href=&quot;#生产者&quot; c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>HTTPS 的思考</title>
    <link href="http://ideajava.com/2019/06/25/HTTPS/"/>
    <id>http://ideajava.com/2019/06/25/HTTPS/</id>
    <published>2019-06-25T02:49:05.000Z</published>
    <updated>2019-06-25T04:47:32.454Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知, HTTPS 是为了解决信息安全传输的问题, 我们来推理一下, 怎么才能达到安全传输的效果.</p><p>比如, A 发信息和 B 聊天, 为了防止聊天内容被截获偷窥, 我们可以怎么做?</p><h3 id="对称加密"><a href="#对称加密" class="headerlink" title="对称加密"></a>对称加密</h3><p>一个比较简单的方式, 就是对聊天内容进行加密, A 发送消息前用密码 XXX 对内容进行加密, B 收到之后对内容用密码 XXX 进行解密, 这种方式称为对称加密. 其他人不知道密码, 当然就没法对内容进行解密.</p><p>但是, A 怎么把密码告诉 B 呢, 如果通过网络传输, 发送的密码被偷窥了, 那也没办法达到安全传输. 当然, A 可以通过线下的方式将密码给到 B, 可是 A 还要跟很多人聊天, 不可能都通过线下的方式.</p><h3 id="非对称加密"><a href="#非对称加密" class="headerlink" title="非对称加密"></a>非对称加密</h3><p>为了解决对称加密的存在的问题, 出现了非对称加密. A 和 B 都有一对公私秘钥对, A 公钥加密的内容只有 A 的私钥才能解密, B 公钥加密的内容只有 B 的私钥才能解密. 这样, A 把自己的公钥给 B, B 把自己的公钥给 A. 聊天的时候:</p><p><code>A 发送的内容 ---&gt; 用 B 的公钥加密 ---&gt; B 接收到加密的内容 ---&gt; B 用自己的私钥解密</code></p><p>这样就能达到安全传输了, 因为 B 的私钥只有自己才有, B 的公钥泄漏了也没关系, 公钥是用来加密的. </p><h3 id="非对称加密-对称加密"><a href="#非对称加密-对称加密" class="headerlink" title="非对称加密 + 对称加密"></a>非对称加密 + 对称加密</h3><p>非对称加密一般比较慢, 如果内容一直都是用对称加密来保护, 那性能就会有影响了. 所以, 我们可以用对称加密的方式来传输对称加密的密钥, 之后就用对称加密来聊天即可.</p><h3 id="中间人夹击"><a href="#中间人夹击" class="headerlink" title="中间人夹击"></a>中间人夹击</h3><p>但是, 非对称加密也没法解决<strong>中间人夹击</strong>的问题.</p><p>试想一下, 在 A 和 B 之间, 有一个中间人. 在 A 和 B 交互公钥的时候, 中间人都把公钥拦截下来了, 把自己的公钥给到 A 和 B. 这样, A 以为自己在和 B 聊天, 其实是在和中间人聊天, B 也一样.</p><p><code>A 发送的内容 ---&gt; 用中间人的公钥加密 ---&gt; 中间人接收到加密的内容 ---&gt; 中间人用自己的私钥解密, 偷看到内容了 ---&gt; 中间人用 B 的公钥加密 ---&gt; B 接收到加密的内容 ---&gt; B 用自己的私钥解密</code></p><h3 id="公钥究竟是谁的"><a href="#公钥究竟是谁的" class="headerlink" title="公钥究竟是谁的?"></a>公钥究竟是谁的?</h3><p>中间人夹击的问题在于公钥被偷天换日了, A 收到的公钥怎么才能知道这确确实实是 B 的公钥, 而不是中间人的?</p><p>现实中, 我们用身份证来表明自己的身份. 那么, 网络中有没有一个公信力的机构, 给 A 和 B 颁发一个”身份证”, 这个”身份证”上有属于自己的公钥. 当 A 拿到 B 的”身份证”的时候, 一看是 B 的, 然后从中拿到公钥.</p><p>这里的”身份证”就是数字证书, 而公信力机构就是 CA.</p><p>但是, 我怎么知道这个”身份证”是真的还是假的呢? 国家的身份证系统能查到身份证的真假, 那么操作系统或浏览器其实也有预置的CA列表来验证这个”身份证”是不是伪造的.</p><h3 id="HTTPS-简化流程"><a href="#HTTPS-简化流程" class="headerlink" title="HTTPS 简化流程"></a>HTTPS 简化流程</h3><ol><li>浏览器发出 HTTPS 请求</li><li>服务器发送数字证书(证书上有服务器的公钥)</li><li>浏览器用预置的CA列表来验证证书是不是伪造的, 如果有风险则提示</li><li>浏览器生成随机的密钥对, 用服务器的公钥加密发送自己的密钥对给服务器</li><li>服务器用自己的私钥解密, 得到浏览器的密钥对</li><li>双方都知道了公钥, 可以开始加密传输了</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;众所周知, HTTPS 是为了解决信息安全传输的问题, 我们来推理一下, 怎么才能达到安全传输的效果.&lt;/p&gt;
&lt;p&gt;比如, A 发信息和 B 聊天, 为了防止聊天内容被截获偷窥, 我们可以怎么做?&lt;/p&gt;
&lt;h3 id=&quot;对称加密&quot;&gt;&lt;a href=&quot;#对称加密&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Spring Kafka AckMode</title>
    <link href="http://ideajava.com/2019/06/20/Spring-Kafka-AckMode/"/>
    <id>http://ideajava.com/2019/06/20/Spring-Kafka-AckMode/</id>
    <published>2019-06-20T09:36:49.000Z</published>
    <updated>2019-11-02T08:32:30.425Z</updated>
    
    <content type="html"><![CDATA[<h2 id="enable-auto-commit"><a href="#enable-auto-commit" class="headerlink" title="enable-auto-commit"></a>enable-auto-commit</h2><ol><li>当 enable-auto-commit 为 true 时, 会根据 auto-commit-interval 配置的时间间隔去自动 commit, 就算 record 被消费异常也会自动 commit.</li><li>当 enable-auto-commit 为 false 时, 需要根据 listener 的 ack-mode 来确定确认模式.</li></ol><h2 id="ack-mode"><a href="#ack-mode" class="headerlink" title="ack-mode"></a>ack-mode</h2><h4 id="record"><a href="#record" class="headerlink" title="record"></a>record</h4><p>在 listener 处理每条消息之后提交, 即处理一条提交一条.</p><h4 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h4><p>在下一次 poll 之前提交已经处理完的记录.</p><h4 id="time"><a href="#time" class="headerlink" title="time"></a>time</h4><p>按照时间间隔来提交, 单位为毫秒.</p><h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>累积到 count 数目时提交.</p><h4 id="count-time"><a href="#count-time" class="headerlink" title="count_time"></a>count_time</h4><p>到了时间或者到了累积的数目时提交.</p><h4 id="manual"><a href="#manual" class="headerlink" title="manual"></a>manual</h4><p>手工提交, 需要在业务代码中调用 Acknowledgment.acknowledge() 提交, 调用之后，就是跟 batch 同理处理。</p><h4 id="manual-immediate"><a href="#manual-immediate" class="headerlink" title="manual_immediate"></a>manual_immediate</h4><p>调用 Acknowledgment.acknowledge() 之后立马提交.</p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>如果你想业务代码发生异常时进行某些处理再提交, 你可能需要将 enable-auto-commit 设置为 false, 然后 ack-mode 设置为手工提交两种模式之一, 其他情况, 都会按照一定方式自动提交.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;enable-auto-commit&quot;&gt;&lt;a href=&quot;#enable-auto-commit&quot; class=&quot;headerlink&quot; title=&quot;enable-auto-commit&quot;&gt;&lt;/a&gt;enable-auto-commit&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>sleuth zipkin kafka elasticsearch 链路追踪</title>
    <link href="http://ideajava.com/2019/06/18/trace/"/>
    <id>http://ideajava.com/2019/06/18/trace/</id>
    <published>2019-06-18T02:05:59.000Z</published>
    <updated>2019-06-18T03:01:22.532Z</updated>
    
    <content type="html"><![CDATA[<p>在一个大型的分布式系统中, 链路追踪是比不可少的, 否则排查问题将会是一个灾难. 关于链路追踪的相关介绍就不展开讨论了, 可以参考下面的链接, 本文着重介绍实际操作.</p><ul><li><a href="https://ai.google/research/pubs/pub36356" target="_blank" rel="noopener">Dapper</a></li><li><a href="https://zipkin.io/" target="_blank" rel="noopener">zipkin</a></li><li><a href="https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html" target="_blank" rel="noopener">Spring Cloud Sleuth</a></li></ul><h4 id="kafka-安装"><a href="#kafka-安装" class="headerlink" title="kafka 安装"></a>kafka 安装</h4><p>参考官方的 <a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">Quick Start</a></p><h4 id="elasticsearch-安装"><a href="#elasticsearch-安装" class="headerlink" title="elasticsearch 安装"></a>elasticsearch 安装</h4><ol><li>下载 <code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.1.1-linux-x86_64.tar.gz</code></li><li>解压 <code>tar -zxf elasticsearch-7.1.1-linux-x86_64.tar.gz</code></li><li>启动 <code>./bin/elasticsearch</code></li><li>检查 <code>curl localhost:9200</code></li></ol><p>如果以 root 用户启动会报错</p><p><code>org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root</code></p><p>因此我们可以创建一个非 root 用户来启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">groupadd es </span><br><span class="line">useradd es -g es -p es</span><br><span class="line">chown -R es:es $&#123;elasticsearch_HOME&#125;/</span><br><span class="line">sudo su es</span><br><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure><h4 id="zipkin-服务"><a href="#zipkin-服务" class="headerlink" title="zipkin 服务"></a>zipkin 服务</h4><ol><li>下载 <code>curl -sSL https://zipkin.io/quickstart.sh | bash -s</code></li><li>启动 <code>STORAGE_TYPE=elasticsearch ES_HOSTS=http://127.0.0.1:9200 KAFKA_BOOTSTRAP_SERVERS=127.0.0.1:9092 java -jar zipkin.jar &amp;</code></li><li>检查 在浏览器中打开 <code>http://127.0.0.1:9411</code></li></ol><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/zipkin.png" alt="zipkin"></p><h4 id="spring-boot-微服务配置"><a href="#spring-boot-微服务配置" class="headerlink" title="spring boot 微服务配置"></a>spring boot 微服务配置</h4><ol><li>pom.xml 需要加入以下依赖</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencyManagement&gt; </span><br><span class="line">      &lt;dependencies&gt;</span><br><span class="line">          &lt;dependency&gt;</span><br><span class="line">              &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</span><br><span class="line">              &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;</span><br><span class="line">              &lt;version&gt;$&#123;release.train.version&#125;&lt;/version&gt;</span><br><span class="line">              &lt;type&gt;pom&lt;/type&gt;</span><br><span class="line">              &lt;scope&gt;import&lt;/scope&gt;</span><br><span class="line">          &lt;/dependency&gt;</span><br><span class="line">      &lt;/dependencies&gt;</span><br><span class="line">&lt;/dependencyManagement&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>application.yml 配置</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  sleuth:</span><br><span class="line">    sampler:</span><br><span class="line">      probability: 1.0</span><br><span class="line">  zipkin:</span><br><span class="line">    sender:</span><br><span class="line">      type: kafka</span><br><span class="line">  kafka:</span><br><span class="line">    bootstrap-servers: 127.0.0.1:9092</span><br></pre></td></tr></table></figure><ol start="3"><li>logback 配置</li></ol><p>在日志的 pattern 中加入 <code>[%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}]</code> 就可以在日志中将 traceId 和 spanId 打印出来了.</p><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>启动两个微服务, 然后发起一个服务之间调用的请求. </p><ol><li>日志的输出应该有类似以下的内容:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-06-18 10:49:46,797 [304b31da6dbe1609,304b31da6dbe1609]</span><br></pre></td></tr></table></figure><ol start="2"><li>根据 traceId 在 zipkin 中搜索</li></ol><p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/zipkin-search.png" alt="zipkin-search"></p><ol start="3"><li>检查内容是不是真的保存在 elasticsearch 中</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; &apos;localhost:9200/_search&apos; -d &apos;&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;&quot;traceId&quot;:&quot;304b31da6dbe1609&quot;&#125;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在一个大型的分布式系统中, 链路追踪是比不可少的, 否则排查问题将会是一个灾难. 关于链路追踪的相关介绍就不展开讨论了, 可以参考下面的链接, 本文着重介绍实际操作.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://ai.google/research/pubs
      
    
    </summary>
    
    
    
      <category term="sleuth zipkin kafka elasticsearch" scheme="http://ideajava.com/tags/sleuth-zipkin-kafka-elasticsearch/"/>
    
  </entry>
  
</feed>
