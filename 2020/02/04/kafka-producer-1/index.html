<!DOCTYPE html>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=0">

  <meta name="description" content="最近花了几天时间阅读了 Kafka producer 的源码，内部的细节问题还是挺多的，还没有一行行细扣，只是看了下主要的脉络，尝试梳理一下其中的逻辑。
主要流程涉及以下几个核心类：

KafkaProducer
RecordAccumulator
Sender
NetworkClient
org.">


<link rel="alternate" href="/atom.xml" title="Rason&#39;s Blog" type="application/atom+xml">
<meta name="theme-color" content="#a1d0f6">
<title>Kafka producer 源码阅读（一） - Rason&#39;s Blog</title>
<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
<link rel="shortcut icon" href="/favicon.png">
<link rel="stylesheet" href="/css/style.css">
<nav class="main-nav">
	
	    <a href="/">← Home</a>
	
	
	    <a href="/about/">About</a>
	
	    <a href="/archives/">Archives</a>
	
</nav>


<section id="wrapper">
    <article class="post">
    <header>
        
            <h1>Kafka producer 源码阅读（一）</h1>
        
        <h2 class="headline">Feb 04 2020
        
        </h2>
    </header>
</article>
<section id="post-body"><p>最近花了几天时间阅读了 Kafka producer 的源码，内部的细节问题还是挺多的，还没有一行行细扣，只是看了下主要的脉络，尝试梳理一下其中的逻辑。</p>
<p>主要流程涉及以下几个核心类：</p>
<ul>
<li>KafkaProducer</li>
<li>RecordAccumulator</li>
<li>Sender</li>
<li>NetworkClient</li>
<li>org.apache.kafka.common.network.Selector</li>
<li>KafkaChannel</li>
</ul>
<p>核心链路可以简单概括为四点：</p>
<ol>
<li>通过 <code>KafkaProducer</code> 发送的消息会先到 <code>RecordAccumulator</code>，RecordAccumulator 是一个消息累积器，我们知道 Kafka 消息是可以批量发送的</li>
<li><code>RecordAccumulator</code> 中的消息批次满了或者新建了一个批次会唤醒 <code>Sender</code> 线程对消息进行发送，当然 <code>linger.ms</code> 时间到了也会对消息进行发送</li>
<li><code>Sender</code> 线程通过 <code>NetworkClient</code> 构造一个发送请求，然后调用 <code>org.apache.kafka.common.network.Selector</code> 的 <code>send</code> 方法注册感兴趣的写事件</li>
<li>随后 <code>NetworkClient</code> 的 <code>poll</code> 方法会调用 <code>org.apache.kafka.common.network.Selector</code> 的 <code>poll</code> 方法把请求真正发出去</li>
</ol>
<h3 id="KafkaProducer"><a href="#KafkaProducer" class="headerlink" title="KafkaProducer"></a>KafkaProducer</h3><p>KafkaProducer 是将消息发布到 Kafka 群集的 Kafka 客户端。producer 是线程安全的，跨线程共享单个 producer 实例通常比拥有多个实例更快。因此，我们使用 Springboot 集成 Kafka 的时候，默认只会创建一个 KafkaProducer 实例。</p>
<p>当我们通过 <code>org.springframework.kafka.core.KafkaTemplate#send</code> 发送消息时，实际会调用到的就是 <code>org.apache.kafka.clients.producer.KafkaProducer#doSend</code> 方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123;</span><br><span class="line">        TopicPartition tp = null;</span><br><span class="line"></span><br><span class="line">        throwIfProducerClosed();</span><br><span class="line">        // 首选确保 topic 的元数据可用</span><br><span class="line">        ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">        </span><br><span class="line">        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">        Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line"></span><br><span class="line">        // key 和 record 序列化</span><br><span class="line">        byte[] serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">        byte[] serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">       </span><br><span class="line">        // 根据发送的 record 计算发送到哪个分区</span><br><span class="line">        int partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">        tp = new TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 确保消息不能太大</span><br><span class="line">        int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">                compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">        ensureValidRecordSize(serializedSize);</span><br><span class="line"></span><br><span class="line">        // 将消息添加到消息累积器</span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line"></span><br><span class="line">        // 如果批次满了或者新建了一个批次，就会唤醒 sender 线程</span><br><span class="line">        if (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">            this.sender.wakeup();</span><br><span class="line">        &#125;</span><br><span class="line">        return result.future;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>代码我精简了部分内容，可以看出发送方法主要就是干了以下几件事情：</p>
<ol>
<li>确保 topic 的元数据可用，如果元数据没有的话会先发起元数据请求，因为 KafkaProducer 需要知道一些元数据才能知道发送到哪个 broker 和哪个分区</li>
<li>对分区 key 和消息 record 进行序列化，我们可以通过 <code>key.serializer</code> 和 <code>value.serializer</code> 两个配置项自定义序列化方式</li>
<li>计算发送到 topic 的哪个分区，默认情况下如果没有指定分区 key，则会通过轮询的方式负载均衡发送到分区，如果指定了分区 key，则根据 key 的哈希对分区数取模计算出分区</li>
<li>确保消息的大小不能大于 <code>max.request.size</code> 和 <code>buffer.memory</code> 配置的值</li>
<li>将消息添加到记录累积器，这一步是整个方法的核心，在 Kafka 中消息的发送是按批次发送的，当然一个批次可以只有一条消息，这一步骤我们下面展开分析</li>
<li>如果批次满了或者新建了一个批次，就会唤醒 sender 线程，sender 线程会对消息进行发送</li>
</ol>
<p>另外，我们留意到 accumulator.append 方法还传入了一个 remainingWaitMs 参数，这是因为 Kafka 提供了一个 <code>max.block.ms</code> 参数来让我们控制 <code>KafkaProducer.send()</code> 和 <code>KafkaProducer.partitionsFor()</code> 方法最长的阻塞时间。</p>
<h3 id="RecordAccumulator"><a href="#RecordAccumulator" class="headerlink" title="RecordAccumulator"></a>RecordAccumulator</h3><p>RecordAccumulator 相当于一个队列，将要发送的记录累积要发送到服务器的 <code>MemoryRecords</code> 实例中。RecordAccumulator 是有内存限制的，当内存不够用的时候，append 就会被阻塞，这就是为什么会有 <code>max.block.ms</code> 这个配置项。</p>
<p>在开始看代码之前，我们先了解一个 RecordAccumulator 保存消息的数据结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches;</span><br></pre></td></tr></table></figure>
<p>也就是说，每个主题的每个分区，都会对应一个双端队列，而队列里面保存的是 ProducerBatch ，ProducerBatch 就是保存一个批次消息的对象。因为 Kafka broker 中的消息是按分区来存储的，所有只有同一个分区的消息才会打成一个批次，这个应该很好理解。</p>
<p>现在，我们来看一下 <code>RecordAccumulator.append</code> 方法： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public RecordAppendResult append(TopicPartition tp,</span><br><span class="line">                                     long timestamp,</span><br><span class="line">                                     byte[] key,</span><br><span class="line">                                     byte[] value,</span><br><span class="line">                                     Header[] headers,</span><br><span class="line">                                     Callback callback,</span><br><span class="line">                                     long maxTimeToBlock) throws InterruptedException &#123;</span><br><span class="line">        ByteBuffer buffer = null;</span><br><span class="line">        if (headers == null) headers = Record.EMPTY_HEADERS;</span><br><span class="line">        try &#123;</span><br><span class="line">            // 检查我们是否有在进行中的 batch ，如果有就添加到一个 ProducerBatch 中</span><br><span class="line">            Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">            synchronized (dq) &#123;</span><br><span class="line">                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">                if (appendResult != null)</span><br><span class="line">                    return appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // 如果没有进行中的 record batch ，那么就创建一个新的 batch，开辟内存的大小为 batch.size 和本条记录的最大值</span><br><span class="line">            byte maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">            int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">            buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line"></span><br><span class="line">            synchronized (dq) &#123;</span><br><span class="line">				// 由于 dq 是同步的，所以在多线程并发的时候，其他线程可能已经帮我们创建出一个新的 ProducerBatch，所以进入同步代码块之后需要再次尝试 append</span><br><span class="line">                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">                if (appendResult != null) &#123;</span><br><span class="line">                    return appendResult;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                // 创建新的 ProducerBatch 并将消息添加到该 ProducerBatch 中</span><br><span class="line">                MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">                ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">                FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">                // 将 ProducerBatch 添加到 Deque 中</span><br><span class="line">                dq.addLast(batch);</span><br><span class="line">                incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">                // Don&apos;t deallocate this buffer in the finally block as it&apos;s being used in the record batch</span><br><span class="line">                buffer = null;</span><br><span class="line">                return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            if (buffer != null)</span><br><span class="line">                free.deallocate(buffer);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>主要逻辑应该还是比较清晰的：</p>
<ol>
<li>首先获取到该 <code>TopicPartition</code> 对应的 <code>Deque&lt;ProducerBatch&gt;</code> ，然后 <code>tryAppend</code> 方法中会取出 Deque<producerbatch> 中最后一个 <code>ProducerBatch</code>，如果不为空则尝试添加到该 ProducerBatch 中，如果为空则继续往下走</producerbatch></li>
<li>没有可用 ProducerBatch ，则创建一个新的，开辟内存的大小为 <code>batch.size</code> 和本条记录的最大值</li>
<li>同步代码块中会再次调用 <code>tryAppend</code> 方法，是因为在多线程并发的时候，其他线程可能已经帮我们创建出一个新的 ProducerBatch</li>
<li>创建新的 ProducerBatch 并将消息添加到该 ProducerBatch 中，消息实际是通过 <code>MemoryRecordsBuilder</code> 写到前面开辟的 <code>buffer</code> 中，其实就是写内存</li>
<li>返回 append 的结果，注意这个结果很重要，因为上面的 KafkaProducer 就是根据这个返回结果判断是否需要唤醒 sender 线程</li>
</ol>
<h3 id="Sender"><a href="#Sender" class="headerlink" title="Sender"></a>Sender</h3><p>Sender 是向 Kafka 集群发送生产请求的后台线程。该线程会发出元数据请求以更新其群集元数据，然后将产生的请求发送到适当的节点。跟踪代码会发现，Sender 线程是在 KafkaProducer 构造方法中启动的。</p>
<p>我们来看一下 Sender 线程的 <code>run</code> 方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line">    // 主循环，直接 close 方法被调用</span><br><span class="line">    while (running) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            runOnce();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void runOnce() &#123;</span><br><span class="line">    long currentTimeMs = time.milliseconds();</span><br><span class="line">    long pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line">    client.poll(pollTimeout, currentTimeMs);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>简单的几行代码，包含的信息量其实是很大的：</p>
<ol>
<li>Sender 是不断地循环的，直到 close 方法被调用，意思就是不断地循环检查有没有数据需要发送</li>
<li><code>sendProducerData</code> 看名字就知道是发送数据</li>
<li><code>poll</code> 方法其实才是真正发生 socket 读写的地方，所以 sendProducerData 其实相当于构造请求，注册写事件</li>
</ol>
<p>接着看一下 sendProducerData 方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">private long sendProducerData(long now) &#123;</span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    // 获取其分区准备好发送的节点列表</span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">    // 如果有哪个分区的 Leader 分区未知，则强制元数据更新</span><br><span class="line">    if (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class="line">        for (String topic : result.unknownLeaderTopics)</span><br><span class="line">            this.metadata.add(topic);</span><br><span class="line">        this.metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 移出掉还没有创建好连接的节点</span><br><span class="line">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">    long notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">    while (iter.hasNext()) &#123;</span><br><span class="line">        Node node = iter.next();</span><br><span class="line">        if (!this.client.ready(node, now)) &#123;</span><br><span class="line">            iter.remove();</span><br><span class="line">            notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 以节点为维度整理出需要发送的 ProducerBatch 列表</span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);</span><br><span class="line">    addToInflightBatches(batches);</span><br><span class="line"></span><br><span class="line">    // 单个分区顺序性保证</span><br><span class="line">    if (guaranteeMessageOrder) &#123;</span><br><span class="line">        for (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123;</span><br><span class="line">            for (ProducerBatch batch : batchList)</span><br><span class="line">                this.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    accumulator.resetNextBatchExpiryTime();</span><br><span class="line">    List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now);</span><br><span class="line">    List&lt;ProducerBatch&gt; expiredBatches = this.accumulator.expiredBatches(now);</span><br><span class="line">    expiredBatches.addAll(expiredInflightBatches);</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">    if (!expiredBatches.isEmpty())</span><br><span class="line">        log.trace(&quot;Expired &#123;&#125; batches in accumulator&quot;, expiredBatches.size());</span><br><span class="line">    for (ProducerBatch expiredBatch : expiredBatches) &#123;</span><br><span class="line">        String errorMessage = &quot;Expiring &quot; + expiredBatch.recordCount + &quot; record(s) for &quot; + expiredBatch.topicPartition</span><br><span class="line">            + &quot;:&quot; + (now - expiredBatch.createdMs) + &quot; ms has passed since batch creation&quot;;</span><br><span class="line">        failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);</span><br><span class="line">        if (transactionManager != null &amp;&amp; expiredBatch.inRetry()) &#123;</span><br><span class="line">            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sensors.updateProduceRequestMetrics(batches);</span><br><span class="line"></span><br><span class="line">    // 如果有任何节点准备发送+具有可发送数据，会使用0超时进行轮询，这样可以立即循环并尝试发送更多数据。</span><br><span class="line">    // 否则，超时将是下一批到期时间与检查数据可用性的延迟时间之间的较小值。</span><br><span class="line">    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">    pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);</span><br><span class="line">    pollTimeout = Math.max(pollTimeout, 0);</span><br><span class="line">    if (!result.readyNodes.isEmpty()) &#123;</span><br><span class="line">        pollTimeout = 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 发送生产请求</span><br><span class="line">    sendProduceRequests(batches, now);</span><br><span class="line">    return pollTimeout;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要逻辑在代码上都有注解，需要解析的是返回值 <code>pollTimeout</code>。这个返回值会用于主循环的 <code>client.poll</code> 方法中，代表 poll 阻塞的时间。</p>
<ul>
<li>如果某个分区已经准备好发送，则阻塞时间为0；</li>
<li>如果某个分区已经积累了一些数据但尚未准备好，则阻塞时间为“现在”与其“延迟到期时间”的时差；</li>
<li>否则，阻塞为“现在”与“元数据到期时间”的时差；</li>
</ul>
<p>所以，当我们设置了 <code>linger.ms</code> 的值时，不需要等到批次满了也会根据这个延迟时间来进行发送。</p>
<p>接着我们看下 <code>sendProduceRequest</code> 方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private void sendProduceRequest(long now, int destination, short acks, int timeout, List&lt;ProducerBatch&gt; batches) &#123;</span><br><span class="line">        ...</span><br><span class="line">        ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,</span><br><span class="line">                requestTimeoutMs, callback);</span><br><span class="line">        client.send(clientRequest, now);</span><br><span class="line">        log.trace(&quot;Sent produce request to &#123;&#125;: &#123;&#125;&quot;, nodeId, requestBuilder);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>简单来讲，就是构造一个客户端请求 <code>ClientRequest</code> ，然后调用客户端的 send 方法（org.apache.kafka.clients.KafkaClient#send）进行发送。</p>
<h3 id="NetworkClient"><a href="#NetworkClient" class="headerlink" title="NetworkClient"></a>NetworkClient</h3><p><code>NetworkClient</code> 是 <code>KafkaClient</code> 的实现类，也就是说上面的 send 方法实际调用的就是 NetworkClient 的 send 方法。</p>
<p>NetworkClient 用于异步请求/响应网络IO的网络客户端，Kafka 的生产者和消费者就是通过这个类来进行网络操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now, AbstractRequest request) &#123;</span><br><span class="line">    String destination = clientRequest.destination();</span><br><span class="line">    RequestHeader header = clientRequest.makeHeader(request.version());</span><br><span class="line">    Send send = request.toSend(destination, header);</span><br><span class="line">    InFlightRequest inFlightRequest = new InFlightRequest(</span><br><span class="line">            clientRequest,</span><br><span class="line">            header,</span><br><span class="line">            isInternalRequest,</span><br><span class="line">            request,</span><br><span class="line">            send,</span><br><span class="line">            now);</span><br><span class="line">    this.inFlightRequests.add(inFlightRequest);</span><br><span class="line">    selector.send(send);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法比较简单，构造一个发送到目标 broker 数据的 Send 实体，然后调用 <code>org.apache.kafka.common.network.Selector#send</code> 方法。</p>
<h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><p>Selector 其实是包装了 Java NIO, 是一个 nioSelector 接口，用于执行无阻塞的多连接网络I/O。与  <code>NetworkSend</code> 和  <code>NetworkReceive</code> 协同工作，以传输网络请求和响应。</p>
<p>我们先看了解一下其用法：</p>
<p>通过执行以下操作，可以创建一个连接添加到 nioSelector</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selector.connect(42, new InetSocketAddress(&quot;google.com&quot;, server.port), 64000, 64000);</span><br></pre></td></tr></table></figure>
<p><code>connect</code> 方法的调用不会阻塞在创建TCP连接，因此 connect 方法仅开始启动连接。成功调用此方法并不意味着已建立有效连接。发送请求、接收响应、处理连接完成以及现有连接上的断开都是使用 <code>poll()</code> 调用完成的。</p>
<p>比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nioSelector.send(new NetworkSend(myDestination, myBytes));</span><br><span class="line">nioSelector.send(new NetworkSend(myOtherDestination, myOtherBytes));</span><br><span class="line">nioSelector.poll(TIMEOUT_MS);</span><br></pre></td></tr></table></figure>
<p>了解 Selector 的用法之后，我们再看回 Sender 主循环的代码应该就是茅塞顿开的感觉：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">void runOnce() &#123;</span><br><span class="line">    long currentTimeMs = time.milliseconds();</span><br><span class="line">    long pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line">    client.poll(pollTimeout, currentTimeMs);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于篇幅关系，到这里 poll 方法就暂不展开分析了。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我好像把总结写在文章开头了。</p>
</section>
    

    <footer id="post-meta" class="clearfix">
        <a href="/about/">
        <img class="avatar" src="/images/avatar.png">
        <div>
            <span class="dark">Rason&#39;s Blog</span>
            <span></span>
        </div>
        </a>
        <section id="sharing">
            <a title="Share to Twitter" class="twitter" href="https://twitter.com/intent/tweet?text=http://ideajava.com/2020/02/04/kafka-producer-1/ - Kafka producer 源码阅读（一） @"><span class="icon-twitter">tweet</span></a>
            <a title="Share to Facebook" class="facebook" href="#" onclick="
                window.open(
                  'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
                  'facebook-share-dialog',
                  'width=626,height=436');
                return false;"><span class="icon-facebook-sign">Share</span>
            </a>
        </section>
    </footer>


	<footer id="footer">
	<div id="social">
		<p class="small">©  rason| Powered by Hexo 
	
	<a href="http://www.beian.miit.gov.cn" target="_blank">粤ICP备17046371号-1</a></p>
	</div>
</footer>

</section>

	<script src="//cdnjs.loli.net/ajax/libs/instantclick/3.0.1/instantclick.min.js" data-no-instant=""></script>
	<script data-no-instant="">
		
		InstantClick.init('mousedown');
	</script>



