<!DOCTYPE html>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=0">

  <meta name="description" content="ELK">


<link rel="alternate" href="/atom.xml" title="Rason&#39;s Blog" type="application/atom+xml">
<meta name="theme-color" content="#a1d0f6">
<title>ELK日志监控系统 - Rason&#39;s Blog</title>
<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
<link rel="shortcut icon" href="/favicon.png">
<link rel="stylesheet" href="/css/style.css">
<nav class="main-nav">
	
	    <a href="/">← Home</a>
	
	
	    <a href="/about/">About</a>
	
	    <a href="/archives/">Archives</a>
	
</nav>


<section id="wrapper">
    <article class="post">
    <header>
        
            <h1>ELK日志监控系统</h1>
        
        <h2 class="headline">Aug 17 2017
        
            
            <a href="/categories/BigData/#BigData">BigData</a>
        
        </h2>
    </header>
</article>
<section id="post-body"><p>毫无疑问，日志对于开发者来说相当重要，通过日志可以追踪系统出现的问题和运行情况。</p>
<p>目前的工作依旧还是直接SSH登陆服务器，然后使用vim来查看日志，我觉得很麻烦，而且只能进行简单的问题跟踪，对日志数据的进一步分析相当困难。</p>
<p>通过搜索了解到ELK是比较流行的日志监控系统，所以就尝试着使用起来。</p>
<h2 id="ELK-stack"><a href="#ELK-stack" class="headerlink" title="ELK stack"></a>ELK stack</h2><p>ELK stack ：</p>
<ul>
<li><p><strong>L</strong>ogstash: 作为数据的收集引擎，从不同的地方收集数据并进行相应的处理，然后输送到存储引擎，比如ElasticSearch。</p>
</li>
<li><p><strong>E</strong>lasticSearch： 提供数据的存储和查询。ES提供全文搜索和分析，底层给予Lucene。默认就是分布式，易扩展。</p>
</li>
<li><p><strong>K</strong>ibana： 数据可视化工具，可以对ElasticSearch中的数据进行查询可视化。还可以进行分析和生成一些图表。</p>
</li>
</ul>
<h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>从上面的介绍可知，使用ELK，我们可以通过一个Web界面来搜索分布式系统的日志数据。但更重要的是，我们还可以通过对数据的处理来对系统进行监控和调查分析。比如：</p>
<ul>
<li><p>统计某个类型的错误出现的次数</p>
</li>
<li><p>统计某个类型的事件数，以确定受问题影响的用户数量</p>
</li>
<li><p>升级系统之后产生的影响</p>
</li>
</ul>
<p>等等。</p>
<h2 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h2><p>Logstash 作为中心的控制器和日志数据路由。</p>
<p>它可以从不同的数据源（使用<strong>input</strong>）对数据进行收集，然后使用<strong>filters</strong>对数据进行格式化等处理，最后数据流向各种输出端点（使用<strong>output</strong>）。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/logstash.png" alt="Logstash"></p>
<p>因此，使用Logstash其实非常简单，就是通过一个配置文件（比如logstash.conf）来对数据管道中的三个部分（<strong>input - filter - output</strong>）进行配置即可。</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>Logstash 可以通过不同的外部插件来从众多的数据源中接收数据，一些通用的比如“file”,”tcp/udp”，或者是一些其它的比如Kafka topic等。下面就是一个<strong>input</strong>配置的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input &#123;  </span><br><span class="line">  tcp &#123;</span><br><span class="line">   port =&gt; 4560</span><br><span class="line">   codec =&gt; json_lines</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过上面的配置，应用系统就可以通过TCP连接Logstash服务器4560端口发送json格式的数据到Logstash中了。</p>
<a id="more"></a>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>Filters 可以对每条日志信息进行实时处理。比如将非结构化的数据进行结构化等。举个例子，下面是一条日志信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;55.3.244.1 GET /index.html 15824 0.043&quot;</span><br></pre></td></tr></table></figure>
<p>配置filter</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">filter &#123;  </span><br><span class="line">  mutate &#123;</span><br><span class="line">    remove_field =&gt; [ &quot;@version&quot; ]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if [type] == &quot;nginx-access&quot; &#123;</span><br><span class="line">    grok &#123;</span><br><span class="line">      match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IP:clientip&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125;</span><br><span class="line">                 %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    geoip &#123;</span><br><span class="line">      source =&gt; &quot;clientip&quot;</span><br><span class="line">      target =&gt; &quot;geoip&quot;</span><br><span class="line">      database =&gt; &quot;/etc/logstash/GeoLiteCity.dat&quot;</span><br><span class="line">      add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][longitude]&#125;&quot; ]</span><br><span class="line">      add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][latitude]&#125;&quot;  ]</span><br><span class="line">    &#125;</span><br><span class="line">    mutate &#123;</span><br><span class="line">      convert =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;float&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的grok，geoip就是一些filter处理插件，Logstash支持很多内置或者外部的插件对日志数据进行实时处理，这些就不展开了。</p>
<h2 id="output"><a href="#output" class="headerlink" title="output"></a>output</h2><p>指定Logstash 处理后的数据输出地点。同样也支持不同的插件将数据输送到不同的地方。通常我们会使用“elasticsearch”插件，但是也经常会输送到标准输出（stdout）或者HadoopFS hdfs等地方。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">output &#123;  </span><br><span class="line">    if ([environment] == &quot;qa&quot; &#123;</span><br><span class="line">        stdout &#123;&#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [ &quot;elasticsearch-host:port&quot; ]</span><br><span class="line">            index =&gt; &quot;elk-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            document_type =&gt; &quot;log&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="将日志数据推送到Logstash中"><a href="#将日志数据推送到Logstash中" class="headerlink" title="将日志数据推送到Logstash中"></a>将日志数据推送到Logstash中</h2><p>有多种方式可以将日志数据推送到Logstash中：</p>
<ul>
<li><p>一种方式是通过<strong>外部日志传输系统</strong>，比如说<a href="https://github.com/elastic/beats" target="_blank" rel="noopener">Beats</a>。</p>
</li>
<li><p>使用Docker容器运行应用程序的可以利用Docker“日志驱动程序”（<a href="https://www.fluentd.org/guides/recipes/docker-logging" target="_blank" rel="noopener">FluentD Docker Logging</a>）的功能捕获stdout输出——<a href="https://geowarin.github.io/spring-boot-logs-in-elastic-search.html" target="_blank" rel="noopener">EFK stach</a>。你可能还需要看一下<a href="https://github.com/gliderlabs/logspout" target="_blank" rel="noopener">Logspout</a>。</p>
</li>
<li><p>通过自定义的<a href="https://github.com/logstash/logstash-logback-encoder" target="_blank" rel="noopener">Logback Appender</a> 直接在应用中将日志信息传输到Logstash中。Java 日志中一个比较常用的日志方案是slf4j+logback，因此在网上找了一个开源项目<a href="https://github.com/logstash/logstash-logback-encoder" target="_blank" rel="noopener">logstash-logback-encoder</a>来提供自定义的Logback Appender。</p>
</li>
</ul>
<h2 id="使用logstash-logback-encoder"><a href="#使用logstash-logback-encoder" class="headerlink" title="使用logstash-logback-encoder"></a>使用logstash-logback-encoder</h2><p>在我们使用自定义的Logback Appender之前，可能会有一些疑问，比如说“当我们将日志数据传输到Logstash中时，Logstash服务器挂了会怎么样？”，“日志传输的调用会不会阻塞？”，“会不会对系统有性能的影响？”答案是否定的，因为数据是通过异步的方式传输到Logstash中的。</p>
<p><a href="https://github.com/logstash/logstash-logback-encoder/blob/master/src/main/java/net/logstash/logback/appender/AbstractLogstashTcpSocketAppender.java" target="_blank" rel="noopener">LogstashTcpSocketAppender</a> 实际上将数据写到<a href="https://github.com/logstash/logstash-logback-encoder/blob/master/src/main/java/net/logstash/logback/appender/AsyncDisruptorAppender.java" target="_blank" rel="noopener">LMAX RingBuffer</a>，RingBuffer 被独立的线程消费将数据推送到Logstash中。所以，即使Logstash服务挂了也不会阻塞我们的应用代码。</p>
<p>下面是一个logback.xml的配置例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;appender name=&quot;stash&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt;</span><br><span class="line">	&lt;destination&gt;10.16.2.30:4560&lt;/destination&gt;</span><br><span class="line"></span><br><span class="line">	&lt;encoder class=&quot;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder&quot;&gt;</span><br><span class="line">		&lt;providers&gt;</span><br><span class="line">			&lt;mdc/&gt; &lt;!-- MDC variables on the Thread will be written as JSON fields--&gt;</span><br><span class="line">			&lt;context/&gt; &lt;!--Outputs entries from logback&apos;s context --&gt;</span><br><span class="line">			&lt;version/&gt; &lt;!-- Logstash json format version, the @version field in the output--&gt;</span><br><span class="line">			&lt;logLevel/&gt;</span><br><span class="line">			&lt;loggerName/&gt;</span><br><span class="line"></span><br><span class="line">			&lt;pattern&gt;</span><br><span class="line">				&lt;pattern&gt;</span><br><span class="line">					&lt;!-- we can add some custom fields to be sent with all the log entries.      --&gt;</span><br><span class="line">					&lt;!--make filtering easier in Logstash--&gt;</span><br><span class="line">					&lt;!--or searching with Kibana--&gt;</span><br><span class="line">                    &#123;</span><br><span class="line">					&quot;appName&quot;: &quot;qhee-webapp-net&quot;,</span><br><span class="line">					&quot;appVersion&quot;: &quot;1.0&quot;</span><br><span class="line">					&#125;</span><br><span class="line">				&lt;/pattern&gt;</span><br><span class="line">			&lt;/pattern&gt;</span><br><span class="line"></span><br><span class="line">			&lt;threadName/&gt;</span><br><span class="line">			&lt;message/&gt;</span><br><span class="line"></span><br><span class="line">			&lt;logstashMarkers/&gt; &lt;!-- Useful so we can add extra information for specific log lines as Markers--&gt;</span><br><span class="line">			&lt;arguments/&gt; &lt;!--or through StructuredArguments--&gt;</span><br><span class="line"></span><br><span class="line">			&lt;stackTrace/&gt;</span><br><span class="line">		&lt;/providers&gt;</span><br><span class="line">	&lt;/encoder&gt;</span><br><span class="line">&lt;/appender&gt;</span><br><span class="line"></span><br><span class="line">&lt;root level=&quot;INFO&quot;&gt;</span><br><span class="line">	&lt;appender-ref ref=&quot;stash&quot; /&gt;</span><br><span class="line">&lt;/root&gt;</span><br></pre></td></tr></table></figure>
<p>更多关于logstash-logback-encoder的详细内容可以见其<a href="https://github.com/logstash/logstash-logback-encoder" target="_blank" rel="noopener">Github</a>。</p>
<h2 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h2><p>ElasticSearch 可能是ELK stack 中最关键的因素，它充当数据库部分。关于其详细用法由于篇幅关系就不展开了，这里就直接从实际应用出发，修改其配置文件<code>elasticsearch.yml</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">network.host: 0.0.0.0</span><br><span class="line">http.port: 9200</span><br></pre></td></tr></table></figure>
<p>配置其host和port监听请求，注意这个端口就是Logstash output 中配置的elasticsearch端口，这样Logstash中的日志数据就可以传输到ElasticSearch中了。</p>
<p>然后就可以通过<code>bin/elasticsearch</code> 命令启动ElasticSearch，启动过程中可能会遇到一些问题，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]</span><br><span class="line">[2]: max number of threads [1024] for user [laixs] is too low, increase to at least [2048]</span><br><span class="line">[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span><br><span class="line">[4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk</span><br></pre></td></tr></table></figure>
<p>问题的描述应该很清晰，按照提示修改其相应的系统配置既可。</p>
<h2 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h2><p>Kibana 是一个可视化工具，使用起来也是相当的简单，主要就是配置读取的ElasticSearch 地址，修改其配置文件<code>kibana.yml</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server.host: &quot;0.0.0.0&quot;</span><br><span class="line">elasticsearch.url: &quot;http://elasticsearch-host:9200&quot;</span><br></pre></td></tr></table></figure>
<p>然后通过<code>bin/kibana</code>命令即可启动Kibana。</p>
<p><img src="https://raw.githubusercontent.com/rason/rason.github.io/master/image/kibana.png" alt="Kibana"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文只是简单地介绍了一下ELK和Java 日志中如何通过自定义的Logback Appender将日志数据发送到Logstash中。</p>
</section>
    
        
        <h2 class="footline">
            <a href="/tags/ELK/#ELK">ELK</a>
        </h2>
    

    <footer id="post-meta" class="clearfix">
        <a href="/about/">
        <img class="avatar" src="/images/avatar.png">
        <div>
            <span class="dark">Rason&#39;s Blog</span>
            <span></span>
        </div>
        </a>
        <section id="sharing">
            <a title="Share to Twitter" class="twitter" href="https://twitter.com/intent/tweet?text=http://ideajava.com/2017/08/17/elk/ - ELK日志监控系统 @"><span class="icon-twitter">tweet</span></a>
            <a title="Share to Facebook" class="facebook" href="#" onclick="
                window.open(
                  'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
                  'facebook-share-dialog',
                  'width=626,height=436');
                return false;"><span class="icon-facebook-sign">Share</span>
            </a>
        </section>
    </footer>


	<footer id="footer">
	<div id="social">
		<p class="small">©  rason| Powered by Hexo 
	
	<a href="http://www.beian.miit.gov.cn" target="_blank">粤ICP备17046371号-1</a></p>
	</div>
</footer>

</section>

	<script src="//cdnjs.loli.net/ajax/libs/instantclick/3.0.1/instantclick.min.js" data-no-instant=""></script>
	<script data-no-instant="">
		
		InstantClick.init('mousedown');
	</script>



